{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2093b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LEGISLATION_URL_PREFIX = os.getenv('LEGISLATION_URL_PREFIX')\n",
    "LEGISLATION_URI_LIST_FILE = os.getenv('LEGISLATION_URI_LIST_FILE')\n",
    "JSON_OUTPUT_DIR = os.getenv('JSON_OUTPUT_DIR', 'json_out')\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.user\n",
      "Warning: Ignoring non-Spark config property: neo4j.database\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Ivy Default Cache set to: /Users/pedroleitao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pedroleitao/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ac8bec2d-f51f-44df-a998-2529e9716c9f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Volumes/Home/pedroleitao/miniconda3/envs/legal-legislation-explorer/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#caniuse-core;1.3.0 in central\n",
      "\tfound org.neo4j#caniuse-api;1.3.0 in central\n",
      "\tfound org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in central\n",
      "\tfound org.jetbrains#annotations;13.0 in central\n",
      "\tfound org.neo4j#caniuse-neo4j-detection;1.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound io.netty#netty-handler;4.1.127.Final in central\n",
      "\tfound io.netty#netty-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.127.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.127.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.73.Final in central\n",
      "\tfound io.projectreactor#reactor-core;3.6.11 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in central\n",
      "\tfound org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in central\n",
      ":: resolution report :: resolve 245ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.netty#netty-buffer;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.73.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.127.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.6.11 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.jetbrains#annotations;13.0 from central in [default]\n",
      "\torg.jetbrains.kotlin#kotlin-stdlib;2.1.20 from central in [default]\n",
      "\torg.neo4j#caniuse-api;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-core;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-neo4j-detection;1.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver-slim;4.4.21 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   0   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ac8bec2d-f51f-44df-a998-2529e9716c9f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/5ms)\n",
      "26/02/19 21:50:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Scala version: 5\n",
      "Neo4j Connector version: 5.3.10_for_spark_3\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark with Neo4j Connector\n",
    "neo4j_maven_pkg = \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.10_for_spark_3\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PSC_Loader_Spark\")\n",
    "    .config(\"spark.jars.packages\", neo4j_maven_pkg)\n",
    "    .config(\"neo4j.url\", NEO4J_URI)\n",
    "    .config(\"neo4j.authentication.basic.user\", NEO4J_USER)\n",
    "    .config(\"neo4j.authentication.basic.password\", NEO4J_PASSWORD)\n",
    "    .config(\"neo4j.database\", NEO4J_DATABASE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Check Spark and Connector versions\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Scala version: {spark.sparkContext.version.split('.')[1]}\")\n",
    "print(f\"Neo4j Connector version: {neo4j_maven_pkg.split(':')[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode_outer, concat, lit, coalesce, md5, to_date, to_timestamp\n",
    "\n",
    "def load_full_hierarchy_to_neo4j(json_dir=f\"{JSON_OUTPUT_DIR}/*.json\"):\n",
    "    \n",
    "    # Read the multi-line JSON files\n",
    "    raw_df = spark.read.option(\"multiline\", \"true\").json(json_dir)\n",
    "    \n",
    "    # Legislation Nodes (With Date Conversions)\n",
    "    print(\"Writing Legislation Nodes...\")\n",
    "    legis_df = raw_df.select(\n",
    "        col(\"legislation_url\").alias(\"uri\"),\n",
    "        col(\"identifier.title\").alias(\"title\"),\n",
    "        col(\"identifier.description\").alias(\"description\"),\n",
    "        to_date(col(\"identifier.modified\"), \"yyyy-MM-dd\").alias(\"modified_date\"),\n",
    "        to_date(col(\"identifier.valid_date\"), \"yyyy-MM-dd\").alias(\"valid_date\"),\n",
    "        to_date(col(\"metadata.enactment_date\"), \"yyyy-MM-dd\").alias(\"enactment_date\")\n",
    "    ).dropDuplicates([\"uri\"])\n",
    "\n",
    "    legis_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MERGE (l:Legislation {uri: row.uri})\n",
    "        SET l.title = row.title, \n",
    "            l.description = row.description,\n",
    "            l.modified_date = row.modified_date,\n",
    "            l.valid_date = row.valid_date,\n",
    "            l.enactment_date = row.enactment_date\n",
    "    \"\"\").save()\n",
    "\n",
    "        \n",
    "    # Part Nodes\n",
    "    print(\"Writing Part Nodes...\")\n",
    "    parts_df = raw_df.select(\n",
    "        col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "        explode_outer(\"parts\").alias(\"part\")\n",
    "    ).filter(col(\"part\").isNotNull()) \\\n",
    "     .withColumn(\"part_num\", col(\"part.part_number\")) \\\n",
    "     .withColumn(\"part_title\", col(\"part.title\")) \\\n",
    "     .withColumn(\"part_id\", concat(col(\"legis_uri\"), lit(\"#part_\"), coalesce(col(\"part_num\"), md5(col(\"part\").cast(\"string\")))))\n",
    "\n",
    "    parts_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (l:Legislation {uri: row.legis_uri})\n",
    "        MERGE (p:Part {id: row.part_id})\n",
    "        SET p.number = row.part_num, p.title = row.part_title\n",
    "        MERGE (l)-[:HAS_PART]->(p)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Chapter Nodes\n",
    "    print(\"Writing Chapter Nodes...\")\n",
    "    chapters_df = parts_df.select(\n",
    "        col(\"part_id\"),\n",
    "        explode_outer(\"part.chapters\").alias(\"chapter\")\n",
    "    ).filter(col(\"chapter\").isNotNull()) \\\n",
    "     .withColumn(\"chapter_num\", col(\"chapter.chapter_number\")) \\\n",
    "     .withColumn(\"chapter_title\", col(\"chapter.title\")) \\\n",
    "     .withColumn(\"chapter_id\", concat(col(\"part_id\"), lit(\"#chapter_\"), coalesce(col(\"chapter_num\"), md5(col(\"chapter\").cast(\"string\")))))\n",
    "\n",
    "    chapters_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (p:Part {id: row.part_id})\n",
    "        MERGE (c:Chapter {id: row.chapter_id})\n",
    "        SET c.number = row.chapter_num, c.title = row.chapter_title\n",
    "        MERGE (p)-[:HAS_CHAPTER]->(c)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Section Nodes (With Safe Date Conversions)\n",
    "    print(\"Writing Section Nodes...\")\n",
    "    sections_df = chapters_df.select(\n",
    "        col(\"chapter_id\"),\n",
    "        explode_outer(\"chapter.sections\").alias(\"section\")\n",
    "    ).filter(col(\"section\").isNotNull()) \\\n",
    "     .withColumn(\"sec_num\", col(\"section.section_number\")) \\\n",
    "     .withColumn(\"sec_title\", col(\"section.title\")) \\\n",
    "     .withColumn(\"sec_uri\", col(\"section.uri\")) \\\n",
    "     .withColumn(\"sec_id\", coalesce(col(\"section.uri\"), concat(col(\"chapter_id\"), lit(\"#sec_\"), coalesce(col(\"sec_num\"), md5(col(\"section\").cast(\"string\"))))))\n",
    "    \n",
    "    # Check if 'valid_start_date' exists in the inferred 'section' struct\n",
    "    section_schema_fields = sections_df.schema[\"section\"].dataType.fieldNames()\n",
    "    if \"valid_start_date\" in section_schema_fields:\n",
    "        sections_df = sections_df.withColumn(\"valid_start_date\", to_date(col(\"section.valid_start_date\"), \"yyyy-MM-dd\"))\n",
    "    else:\n",
    "        sections_df = sections_df.withColumn(\"valid_start_date\", lit(None).cast(\"date\"))\n",
    "\n",
    "    sections_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (c:Chapter {id: row.chapter_id})\n",
    "        MERGE (s:Section {id: row.sec_id})\n",
    "        SET s.number = row.sec_num, \n",
    "            s.title = row.sec_title, \n",
    "            s.uri = row.sec_uri,\n",
    "            s.valid_start_date = row.valid_start_date\n",
    "        MERGE (c)-[:HAS_SECTION]->(s)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Paragraph Nodes (With Safe Date Conversions)\n",
    "    print(\"Writing Paragraph Nodes...\")\n",
    "    paragraphs_df = sections_df.select(\n",
    "        col(\"sec_id\"),\n",
    "        explode_outer(\"section.paragraphs\").alias(\"paragraph\")\n",
    "    ).filter(col(\"paragraph\").isNotNull()) \\\n",
    "     .withColumn(\"para_num\", col(\"paragraph.paragraph_number\")) \\\n",
    "     .withColumn(\"para_text\", col(\"paragraph.text\")) \\\n",
    "     .withColumn(\"para_uri\", col(\"paragraph.uri\")) \\\n",
    "     .withColumn(\"para_id\", coalesce(col(\"paragraph.uri\"), concat(col(\"sec_id\"), lit(\"#para_\"), coalesce(col(\"para_num\"), md5(col(\"paragraph\").cast(\"string\"))))))\n",
    "    \n",
    "    # Check if 'valid_start_date' exists in the inferred 'paragraph' struct\n",
    "    paragraph_schema_fields = paragraphs_df.schema[\"paragraph\"].dataType.fieldNames()\n",
    "    if \"valid_start_date\" in paragraph_schema_fields:\n",
    "        paragraphs_df = paragraphs_df.withColumn(\"valid_start_date\", to_date(col(\"paragraph.valid_start_date\"), \"yyyy-MM-dd\"))\n",
    "    else:\n",
    "        paragraphs_df = paragraphs_df.withColumn(\"valid_start_date\", lit(None).cast(\"date\"))\n",
    "\n",
    "    paragraphs_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (s:Section {id: row.sec_id})\n",
    "        MERGE (pa:Paragraph {id: row.para_id})\n",
    "        SET pa.number = row.para_num, \n",
    "            pa.text = row.para_text, \n",
    "            pa.uri = row.para_uri,\n",
    "            pa.valid_start_date = row.valid_start_date\n",
    "        MERGE (s)-[:HAS_PARAGRAPH]->(pa)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Commentary Nodes\n",
    "    print(\"Writing Commentary Nodes...\")\n",
    "    \n",
    "    # 6a. Commentaries linked to Sections\n",
    "    sec_comm_df = sections_df.select(\n",
    "        col(\"sec_id\").alias(\"parent_id\"),\n",
    "        explode_outer(\"section.commentaries\").alias(\"commentary\")\n",
    "    ).filter(col(\"commentary\").isNotNull())\n",
    "    \n",
    "    sec_comm_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        // FIREWALL: Skip any row that managed to pass through with a null ID\n",
    "        WITH row WHERE row.commentary.ref_id IS NOT NULL\n",
    "        \n",
    "        MATCH (s:Section {id: row.parent_id})\n",
    "        MERGE (com:Commentary {id: row.commentary.ref_id})\n",
    "        SET com.type = row.commentary.type, com.text = row.commentary.text\n",
    "        MERGE (s)-[:HAS_COMMENTARY]->(com)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # 6b. Commentaries linked to Paragraphs\n",
    "    para_comm_df = paragraphs_df.select(\n",
    "        col(\"para_id\").alias(\"parent_id\"),\n",
    "        explode_outer(\"paragraph.commentaries\").alias(\"commentary\")\n",
    "    ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "    para_comm_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        WITH row WHERE row.commentary.ref_id IS NOT NULL\n",
    "        \n",
    "        MATCH (pa:Paragraph {id: row.parent_id})\n",
    "        MERGE (com:Commentary {id: row.commentary.ref_id})\n",
    "        SET com.type = row.commentary.type, com.text = row.commentary.text\n",
    "        MERGE (pa)-[:HAS_COMMENTARY]->(com)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Combine all commentaries for downstream Citation and SubRef processing\n",
    "    all_commentaries_df = sec_comm_df.unionByName(para_comm_df).select(\"commentary\")\n",
    "\n",
    "    # Citation Nodes & Edges\n",
    "    print(\"Writing Citation Nodes...\")\n",
    "    citations_df = all_commentaries_df.select(\n",
    "        col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "        explode_outer(\"commentary.citations\").alias(\"citation\")\n",
    "    ).filter(col(\"citation\").isNotNull())\n",
    "\n",
    "    citations_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        WITH row WHERE row.comm_id IS NOT NULL AND row.citation.uri IS NOT NULL\n",
    "        \n",
    "        MATCH (com:Commentary {id: row.comm_id})\n",
    "        MERGE (cit:Legislation {uri: row.citation.uri})\n",
    "        ON CREATE SET cit.title = row.citation.title, cit.year = row.citation.year, cit.class = row.citation.class\n",
    "        MERGE (com)-[:CITES]->(cit)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Citation SubRef Nodes & Edges\n",
    "    print(\"Writing Citation SubRefs...\")\n",
    "    subrefs_df = all_commentaries_df.select(\n",
    "        col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "        explode_outer(\"commentary.citation_subrefs\").alias(\"subref\")\n",
    "    ).filter(col(\"subref\").isNotNull())\n",
    "\n",
    "    subrefs_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        WITH row WHERE row.comm_id IS NOT NULL AND row.subref.id IS NOT NULL\n",
    "        \n",
    "        MATCH (com:Commentary {id: row.comm_id})\n",
    "        MERGE (sub:CitationSubRef {id: row.subref.id})\n",
    "        SET sub.uri = row.subref.uri, \n",
    "            sub.section_ref = row.subref.section_ref, \n",
    "            sub.text = row.subref.text\n",
    "        MERGE (com)-[:HAS_SUBREF]->(sub)\n",
    "        \n",
    "        WITH sub, row\n",
    "        WHERE row.subref.uri IS NOT NULL\n",
    "        MERGE (leg:Legislation {uri: row.subref.uri})\n",
    "        MERGE (sub)-[:REFERENCES]->(leg)\n",
    "    \"\"\").save()\n",
    "\n",
    "    print(\"Graph load complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5af65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def setup_neo4j_constraints(uri, user, password, database):\n",
    "    \"\"\"\n",
    "    Connects directly to Neo4j to ensure unique constraints exist \n",
    "    before Spark starts pushing data.\n",
    "    \"\"\"\n",
    "    print(\"Setting up Neo4j constraints...\")\n",
    "    constraints = [\n",
    "        \"CREATE CONSTRAINT leg_uri_unique IF NOT EXISTS FOR (l:Legislation) REQUIRE l.uri IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT part_id_unique IF NOT EXISTS FOR (p:Part) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT chap_id_unique IF NOT EXISTS FOR (c:Chapter) REQUIRE c.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sec_id_unique IF NOT EXISTS FOR (s:Section) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT para_id_unique IF NOT EXISTS FOR (pa:Paragraph) REQUIRE pa.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT com_id_unique IF NOT EXISTS FOR (com:Commentary) REQUIRE com.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sub_id_unique IF NOT EXISTS FOR (sub:CitationSubRef) REQUIRE sub.id IS UNIQUE;\"\n",
    "    ]\n",
    "    \n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session(database=database) as session:\n",
    "        for query in constraints:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "    print(\"Constraints successfully applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5e23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neo4j constraints...\n",
      "Constraints successfully applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Legislation Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Part Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Chapter Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Section Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Commentary Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation SubRefs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============================>                       (76 + 12) / 136]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph load complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "setup_neo4j_constraints(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE)\n",
    "load_full_hierarchy_to_neo4j()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-legislation-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
