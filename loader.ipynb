{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2093b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LEGISLATION_URL_PREFIX = os.getenv('LEGISLATION_URL_PREFIX')\n",
    "LEGISLATION_URI_LIST_FILE = os.getenv('LEGISLATION_URI_LIST_FILE')\n",
    "JSON_OUTPUT_DIR = os.getenv('JSON_OUTPUT_DIR', 'json_out')\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d08ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Scala version: 5\n",
      "Neo4j Connector version: 5.3.10_for_spark_3\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark with Neo4j Connector\n",
    "neo4j_maven_pkg = \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.10_for_spark_3\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PSC_Loader_Spark\")\n",
    "    .config(\"spark.jars.packages\", neo4j_maven_pkg)\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"neo4j.url\", NEO4J_URI)\n",
    "    .config(\"neo4j.authentication.basic.user\", NEO4J_USER)\n",
    "    .config(\"neo4j.authentication.basic.password\", NEO4J_PASSWORD)\n",
    "    .config(\"neo4j.database\", NEO4J_DATABASE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Check Spark and Connector versions\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Scala version: {spark.sparkContext.version.split('.')[1]}\")\n",
    "print(f\"Neo4j Connector version: {neo4j_maven_pkg.split(':')[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60bbe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, explode_outer, concat, lit, coalesce, md5, to_date, regexp_replace\n",
    "\n",
    "class LegislationGraphLoader:\n",
    "    def __init__(self, uri, user, password, json_output_dir):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.json_output_dir = json_output_dir\n",
    "\n",
    "    def _write_legislation_nodes(self, raw_df):\n",
    "        print(\"Writing Legislation Nodes...\")\n",
    "        select_exprs = [\n",
    "            col(\"legislation_url\").alias(\"uri\"),\n",
    "            col(\"identifier.title\").alias(\"title\"),\n",
    "            col(\"identifier.description\").alias(\"description\")\n",
    "        ]\n",
    "        \n",
    "        if \"identifier\" in raw_df.columns and \"modified\" in raw_df.schema[\"identifier\"].dataType.fieldNames():\n",
    "            select_exprs.append(to_date(col(\"identifier.modified\"), \"yyyy-MM-dd\").alias(\"modified_date\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"modified_date\"))\n",
    "            \n",
    "        if \"identifier\" in raw_df.columns and \"valid_date\" in raw_df.schema[\"identifier\"].dataType.fieldNames():\n",
    "            select_exprs.append(to_date(col(\"identifier.valid_date\"), \"yyyy-MM-dd\").alias(\"valid_date\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"valid_date\"))\n",
    "\n",
    "        if \"metadata\" in raw_df.columns and \"enactment_date\" in raw_df.schema[\"metadata\"].dataType.fieldNames():\n",
    "            select_exprs.append(to_date(col(\"metadata.enactment_date\"), \"yyyy-MM-dd\").alias(\"enactment_date\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"enactment_date\"))\n",
    "\n",
    "        if \"metadata\" in raw_df.columns and \"status\" in raw_df.schema[\"metadata\"].dataType.fieldNames():\n",
    "            select_exprs.append(col(\"metadata.status\").alias(\"status\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).alias(\"status\"))\n",
    "\n",
    "        if \"metadata\" in raw_df.columns and \"category\" in raw_df.schema[\"metadata\"].dataType.fieldNames():\n",
    "            select_exprs.append(col(\"metadata.category\").alias(\"category\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).alias(\"category\"))\n",
    "\n",
    "        if \"metadata\" in raw_df.columns and \"coming_into_force\" in raw_df.schema[\"metadata\"].dataType.fieldNames():\n",
    "            select_exprs.append(to_date(col(\"metadata.coming_into_force\"), \"yyyy-MM-dd\").alias(\"coming_into_force\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"coming_into_force\"))\n",
    "\n",
    "        legis_df = raw_df.select(*select_exprs).dropDuplicates([\"uri\"])\n",
    "\n",
    "        legis_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MERGE (l:Legislation {uri: row.uri})\n",
    "            SET l.title = row.title, \n",
    "                l.description = row.description,\n",
    "                l.modified_date = row.modified_date,\n",
    "                l.valid_date = row.valid_date,\n",
    "                l.enactment_date = row.enactment_date,\n",
    "                l.status = row.status,\n",
    "                l.category = row.category,\n",
    "                l.coming_into_force = row.coming_into_force\n",
    "        \"\"\").save()\n",
    "\n",
    "    def _write_part_nodes(self, raw_df):\n",
    "        print(\"Writing Part Nodes...\")\n",
    "        parts_df = raw_df.select(\n",
    "            col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "            explode_outer(\"parts\").alias(\"part\")\n",
    "        ).filter(col(\"part\").isNotNull()) \\\n",
    "         .withColumn(\"part_id\", concat(col(\"legis_uri\"), lit(\"#part_\"), coalesce(col(\"part.part_number\"), md5(col(\"part\").cast(\"string\")))))\n",
    "\n",
    "        parts_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (p:Part {id: row.part_id})\n",
    "            SET p.number = row.`part.part_number`,\n",
    "                p.order = row.`part.order`,\n",
    "                p.title = row.`part.title`,\n",
    "                p.uri = row.`part.uri`,\n",
    "                p.status = row.`part.status`,\n",
    "                p.restrict_start_date = row.`part.restrict_start_date`,\n",
    "                p.restrict_end_date = row.`part.restrict_end_date`\n",
    "            MERGE (l)-[:HAS_PART]->(p)\n",
    "        \"\"\").save()\n",
    "        return parts_df\n",
    "\n",
    "    def _write_chapter_nodes(self, parts_df):\n",
    "        print(\"Writing Chapter Nodes...\")\n",
    "        chapters_df = parts_df.select(\n",
    "            col(\"part_id\"),\n",
    "            explode_outer(\"part.chapters\").alias(\"chapter\")\n",
    "        ).filter(col(\"chapter\").isNotNull()) \\\n",
    "         .withColumn(\"chapter_id\", coalesce(col(\"chapter.uri\"), concat(col(\"part_id\"), lit(\"#chapter_\"), coalesce(col(\"chapter.chapter_number\"), md5(col(\"chapter\").cast(\"string\"))))))\n",
    "\n",
    "        chapters_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (p:Part {id: row.part_id})\n",
    "            MERGE (c:Chapter {id: row.chapter_id})\n",
    "            SET c.number = row.`chapter.chapter_number`, \n",
    "                c.order = row.`chapter.order`,\n",
    "                c.title = row.`chapter.title`,\n",
    "                c.uri = row.`chapter.uri`,\n",
    "                c.status = row.`chapter.status`,\n",
    "                c.restrict_start_date = date(row.`chapter.restrict_start_date`),\n",
    "                c.restrict_end_date = date(row.`chapter.restrict_end_date`)\n",
    "            MERGE (p)-[:HAS_CHAPTER]->(c)\n",
    "        \"\"\").save()\n",
    "        return chapters_df\n",
    "\n",
    "    def _write_section_nodes(self, chapters_df):\n",
    "        print(\"Writing Section Nodes...\")\n",
    "        sections_df = chapters_df.select(\n",
    "            col(\"chapter_id\"),\n",
    "            explode_outer(\"chapter.sections\").alias(\"section\")\n",
    "        ).filter(col(\"section\").isNotNull()) \\\n",
    "         .withColumn(\"sec_id\", coalesce(col(\"section.uri\"), concat(col(\"chapter_id\"), lit(\"#sec_\"), coalesce(col(\"section.section_number\"), md5(col(\"section\").cast(\"string\"))))))\n",
    "        \n",
    "        sections_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (c:Chapter {id: row.chapter_id})\n",
    "            MERGE (s:Section {id: row.sec_id})\n",
    "            SET s.number = row.`section.section_number`, \n",
    "                s.order = row.`section.order`,\n",
    "                s.title = row.`section.title`, \n",
    "                s.uri = row.`section.uri`,\n",
    "                s.restrict_extent = row.`section.restrict_extent`,\n",
    "                s.restrict_start_date = date(row.`section.restrict_start_date`),\n",
    "                s.restrict_end_date = date(row.`section.restrict_end_date`)\n",
    "            MERGE (c)-[:HAS_SECTION]->(s)\n",
    "        \"\"\").save()\n",
    "        return sections_df\n",
    "\n",
    "    def _write_paragraph_nodes(self, sections_df):\n",
    "        print(\"Writing Paragraph Nodes...\")\n",
    "        paragraphs_df = sections_df.select(\n",
    "            col(\"sec_id\"),\n",
    "            explode_outer(\"section.paragraphs\").alias(\"paragraph\")\n",
    "        ).filter(col(\"paragraph\").isNotNull()) \\\n",
    "         .withColumn(\"para_id\", coalesce(col(\"paragraph.uri\"), concat(col(\"sec_id\"), lit(\"#para_\"), coalesce(col(\"paragraph.paragraph_number\"), md5(col(\"paragraph\").cast(\"string\"))))))\n",
    "\n",
    "        paragraphs_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (s:Section {id: row.sec_id})\n",
    "            MERGE (pa:Paragraph {id: row.para_id})\n",
    "            SET pa.number = row.`paragraph.paragraph_number`,\n",
    "                pa.order = row.`paragraph.order`,\n",
    "                pa.text = row.`paragraph.text`, \n",
    "                pa.uri = row.`paragraph.uri`\n",
    "            MERGE (s)-[:HAS_PARAGRAPH]->(pa)\n",
    "        \"\"\").save()\n",
    "        return paragraphs_df\n",
    "\n",
    "    def _write_schedules_nodes(self, raw_df):\n",
    "        if \"schedules\" not in raw_df.columns or raw_df.schema[\"schedules\"].dataType.simpleString() == 'array<string>':\n",
    "            return None, None, None\n",
    "\n",
    "        print(\"Writing Schedule Nodes...\")\n",
    "        schedules_df = raw_df.select(\n",
    "            col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "            explode_outer(\"schedules\").alias(\"schedule\")\n",
    "        ).filter(col(\"schedule\").isNotNull()) \\\n",
    "         .withColumn(\"sched_id\", coalesce(col(\"schedule.uri\"), concat(col(\"legis_uri\"), lit(\"#sched_\"), coalesce(col(\"schedule.schedule_number\"), md5(col(\"schedule\").cast(\"string\"))))))\n",
    "         \n",
    "        schedules_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (sc:Schedule {id: row.sched_id})\n",
    "            SET sc.number = row.`schedule.schedule_number`,\n",
    "                sc.order = row.`schedule.order`,\n",
    "                sc.title = row.`schedule.title`,\n",
    "                sc.reference = row.`schedule.reference`,\n",
    "                sc.uri = row.`schedule.uri`\n",
    "            MERGE (l)-[:HAS_SCHEDULE]->(sc)\n",
    "        \"\"\").save()\n",
    "\n",
    "        print(\"Writing Schedule Paragraph Nodes...\")\n",
    "        sched_paras_df = schedules_df.select(\n",
    "            col(\"sched_id\"),\n",
    "            explode_outer(\"schedule.paragraphs\").alias(\"paragraph\")\n",
    "        ).filter(col(\"paragraph\").isNotNull()) \\\n",
    "         .withColumn(\"para_id\", coalesce(col(\"paragraph.uri\"), concat(col(\"sched_id\"), lit(\"#spara_\"), coalesce(col(\"paragraph.paragraph_number\"), md5(col(\"paragraph\").cast(\"string\"))))))\n",
    "\n",
    "        sched_paras_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (sc:Schedule {id: row.sched_id})\n",
    "            MERGE (p:ScheduleParagraph {id: row.para_id})\n",
    "            SET p.number = row.`paragraph.paragraph_number`,\n",
    "                p.order = row.`paragraph.order`,\n",
    "                p.crossheading = row.`paragraph.crossheading`,\n",
    "                p.text = row.`paragraph.text`,\n",
    "                p.uri = row.`paragraph.uri`\n",
    "            MERGE (sc)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\").save()\n",
    "\n",
    "        sched_para_comm_df = sched_paras_df.select(col(\"para_id\").alias(\"parent_id\"), explode_outer(\"paragraph.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        sched_subpara_comm_df = None\n",
    "        if \"subparagraphs\" in sched_paras_df.schema[\"paragraph\"].dataType.fieldNames():\n",
    "            print(\"Writing Schedule Sub-paragraph Nodes...\")\n",
    "            sched_subparas_df = sched_paras_df.select(\n",
    "                col(\"para_id\"),\n",
    "                explode_outer(\"paragraph.subparagraphs\").alias(\"subparagraph\")\n",
    "            ).filter(col(\"subparagraph\").isNotNull()) \\\n",
    "             .withColumn(\"subpara_id\", coalesce(col(\"subparagraph.uri\"), concat(col(\"para_id\"), lit(\"#ssub_\"), coalesce(col(\"subparagraph.subparagraph_number\"), md5(col(\"subparagraph\").cast(\"string\"))))))\n",
    "\n",
    "            sched_subparas_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "                UNWIND event AS row\n",
    "                MATCH (p:ScheduleParagraph {id: row.para_id})\n",
    "                MERGE (sp:ScheduleSubparagraph {id: row.subpara_id})\n",
    "                SET sp.number = row.`subparagraph.subparagraph_number`,\n",
    "                    sp.order = row.`subparagraph.order`,\n",
    "                    sp.text = row.`subparagraph.text`,\n",
    "                    sp.uri = row.`subparagraph.uri`\n",
    "                MERGE (p)-[:HAS_SUBPARAGRAPH]->(sp)\n",
    "            \"\"\").save()\n",
    "\n",
    "            sched_subpara_comm_df = sched_subparas_df.select(col(\"subpara_id\").alias(\"parent_id\"), explode_outer(\"subparagraph.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        return sched_paras_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "\n",
    "    def _write_single_commentary(self, df, parent_label):\n",
    "        if df is not None:\n",
    "            fields = df.schema[\"commentary\"].dataType.fieldNames()\n",
    "            type_col = col(\"commentary.type\") if \"type\" in fields else lit(None)\n",
    "            text_col = col(\"commentary.text\") if \"text\" in fields else lit(None)\n",
    "            \n",
    "            flat_df = df.select(\n",
    "                col(\"parent_id\"),\n",
    "                col(\"commentary.ref_id\").alias(\"ref_id\"),\n",
    "                type_col.alias(\"type\"),\n",
    "                text_col.alias(\"text\")\n",
    "            ).filter(col(\"ref_id\").isNotNull()).dropDuplicates([\"parent_id\", \"ref_id\"])\n",
    "            \n",
    "            flat_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", f\"\"\"\n",
    "                UNWIND event AS row\n",
    "                WITH row WHERE row.ref_id IS NOT NULL\n",
    "                MATCH (parent:{parent_label} {{id: row.parent_id}})\n",
    "                MERGE (com:Commentary {{id: row.ref_id}})\n",
    "                SET com.type = row.type, com.text = row.text\n",
    "                MERGE (parent)-[:HAS_COMMENTARY]->(com)\n",
    "            \"\"\").save()\n",
    "\n",
    "    def _write_commentaries(self, sec_comm_df, para_comm_df, sched_para_comm_df, sched_subpara_comm_df):\n",
    "        print(\"Writing Commentary Nodes...\")\n",
    "        self._write_single_commentary(sec_comm_df, \"Section\")\n",
    "        self._write_single_commentary(para_comm_df, \"Paragraph\")\n",
    "        self._write_single_commentary(sched_para_comm_df, \"ScheduleParagraph\")\n",
    "        self._write_single_commentary(sched_subpara_comm_df, \"ScheduleSubparagraph\")\n",
    "\n",
    "    def _write_citations(self, all_comms):\n",
    "        comm_fields = all_comms.schema[\"commentary\"].dataType.fieldNames()\n",
    "        if \"citations\" not in comm_fields:\n",
    "            return\n",
    "            \n",
    "        print(\"Writing Citation Nodes... (Sequential & Strict Match)\")\n",
    "        citations_df = all_comms.select(\n",
    "            col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "            explode_outer(\"commentary.citations\").alias(\"citation\")\n",
    "        ).filter(col(\"citation\").isNotNull()) \\\n",
    "         .filter(col(\"citation.uri\").isNotNull())\n",
    "        \n",
    "        cit_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "        \n",
    "        citations_flat = citations_df.select(\n",
    "            col(\"comm_id\"),\n",
    "            col(\"citation.id\").alias(\"cit_id\") if \"id\" in cit_fields else lit(None).alias(\"cit_id\"),\n",
    "            col(\"citation.uri\").alias(\"cit_uri\") if \"uri\" in cit_fields else lit(None).alias(\"cit_uri\"),\n",
    "            col(\"citation.title\").alias(\"cit_title\") if \"title\" in cit_fields else lit(None).alias(\"cit_title\"),\n",
    "            col(\"citation.year\").alias(\"cit_year\") if \"year\" in cit_fields else lit(None).alias(\"cit_year\"),\n",
    "            col(\"citation.class\").alias(\"cit_class\") if \"class\" in cit_fields else lit(None).alias(\"cit_class\"),\n",
    "            col(\"citation.text\").alias(\"cit_text\") if \"text\" in cit_fields else lit(None).alias(\"cit_text\")\n",
    "        ).filter(col(\"cit_id\").isNotNull()).dropDuplicates([\"comm_id\", \"cit_id\"])\n",
    "\n",
    "        citations_flat = citations_flat.withColumn(\"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\"))\n",
    "        citations_flat = citations_flat.coalesce(1)\n",
    "\n",
    "        citations_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL AND row.cit_id IS NOT NULL\n",
    "            \n",
    "            MATCH (com:Commentary {id: row.comm_id})\n",
    "            MERGE (cit:Citation {id: row.cit_id})\n",
    "            SET cit.uri = row.cit_uri,\n",
    "                cit.title = row.cit_title,\n",
    "                cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class,\n",
    "                cit.text = row.cit_text\n",
    "            MERGE (com)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            WITH cit, row\n",
    "            WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\").save()\n",
    "\n",
    "    def _write_citation_subrefs(self, all_comms):\n",
    "        comm_fields = all_comms.schema[\"commentary\"].dataType.fieldNames()\n",
    "        if \"citation_subrefs\" not in comm_fields:\n",
    "            return\n",
    "            \n",
    "        print(\"Writing Citation SubRefs... (Sequential & Strict Match)\")\n",
    "        subrefs_df = all_comms.select(\n",
    "            col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "            explode_outer(\"commentary.citation_subrefs\").alias(\"subref\")\n",
    "        ).filter(col(\"subref\").isNotNull()) \\\n",
    "         .filter(col(\"subref.uri\").isNotNull())\n",
    "        \n",
    "        sub_fields = subrefs_df.schema[\"subref\"].dataType.fieldNames()\n",
    "\n",
    "        subrefs_flat = subrefs_df.select(\n",
    "            col(\"comm_id\"),\n",
    "            col(\"subref.id\").alias(\"sub_id\") if \"id\" in sub_fields else lit(None).alias(\"sub_id\"),\n",
    "            col(\"subref.citation_ref\").alias(\"citation_ref\") if \"citation_ref\" in sub_fields else lit(None).alias(\"citation_ref\"),\n",
    "            col(\"subref.uri\").alias(\"sub_uri\") if \"uri\" in sub_fields else lit(None).alias(\"sub_uri\"),\n",
    "            col(\"subref.section_ref\").alias(\"sub_section_ref\") if \"section_ref\" in sub_fields else lit(None).alias(\"sub_section_ref\"),\n",
    "            col(\"subref.text\").alias(\"sub_text\") if \"text\" in sub_fields else lit(None).alias(\"sub_text\")\n",
    "        ).filter(col(\"sub_id\").isNotNull()).dropDuplicates([\"comm_id\", \"sub_id\"])\n",
    "\n",
    "        subrefs_flat = subrefs_flat.withColumn(\"base_uri\", \n",
    "            regexp_replace(col(\"sub_uri\"), r\"(http://www\\.legislation\\.gov\\.uk)/id/([^/]+/[0-9]+/[0-9]+).*\", \"$1/$2\")\n",
    "        )\n",
    "        subrefs_flat = subrefs_flat.coalesce(1)\n",
    "\n",
    "        subrefs_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL AND row.sub_id IS NOT NULL\n",
    "            \n",
    "            MERGE (sub:CitationSubRef {id: row.sub_id})\n",
    "            SET sub.uri = row.sub_uri, \n",
    "                sub.section_ref = row.sub_section_ref, \n",
    "                sub.text = row.sub_text\n",
    "                \n",
    "            WITH sub, row\n",
    "            MATCH (com:Commentary {id: row.comm_id})\n",
    "            OPTIONAL MATCH (cit:Citation {id: row.citation_ref})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN cit IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (cit)-[:HAS_SUBREF]->(sub)\n",
    "            )\n",
    "            FOREACH (_ IN CASE WHEN cit IS NULL THEN [1] ELSE [] END |\n",
    "                MERGE (com)-[:HAS_SUBREF]->(sub)\n",
    "            )\n",
    "            \n",
    "            WITH sub, row\n",
    "            WHERE row.base_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.base_uri})\n",
    "            MERGE (sub)-[:REFERENCES]->(leg)\n",
    "        \"\"\").save()\n",
    "\n",
    "    def _write_super_relationships(self, raw_df):\n",
    "        if \"super\" not in raw_df.columns:\n",
    "            return\n",
    "\n",
    "        print(\"Writing Super Relationships...\")\n",
    "        super_fields = raw_df.schema[\"super\"].dataType.fieldNames()\n",
    "        super_df = raw_df.select(\n",
    "            col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "            col(\"super.supersedes\").alias(\"supersedes\") if \"supersedes\" in super_fields else lit(None).alias(\"supersedes\"),\n",
    "            col(\"super.superseded_by\").alias(\"superseded_by\") if \"superseded_by\" in super_fields else lit(None).alias(\"superseded_by\")\n",
    "        )\n",
    "\n",
    "        super_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"            UNWIND event AS row\n",
    "            WITH row WHERE row.legis_uri IS NOT NULL\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN row.supersedes IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (target:Legislation {uri: row.supersedes})\n",
    "                MERGE (l)-[:SUPERSEDES]->(target)\n",
    "            )\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN row.superseded_by IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (target:Legislation {uri: row.superseded_by})\n",
    "                MERGE (l)-[:SUPERSEDED_BY]->(target)\n",
    "            )\n",
    "        \"\"\").save()\n",
    "\n",
    "    def _write_explanatory_notes_nodes(self, raw_df):\n",
    "        if \"explanatory_notes\" not in raw_df.columns:\n",
    "            return None\n",
    "            \n",
    "        print(\"Writing Explanatory Notes Nodes...\")\n",
    "        notes_base_df = raw_df.select(\n",
    "            col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "            col(\"explanatory_notes\")\n",
    "        ).filter(col(\"explanatory_notes\").isNotNull()) \\\n",
    "         .withColumn(\"notes_id\", coalesce(col(\"explanatory_notes.uri\"), md5(col(\"explanatory_notes\").cast(\"string\"))))\n",
    "        \n",
    "        notes_base_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri}) \n",
    "            MERGE (en:ExplanatoryNotes {id: row.notes_id})\n",
    "            SET en.uri = row.`explanatory_notes.uri`\n",
    "            MERGE (l)-[:HAS_EXPLANATORY_NOTES]->(en)\n",
    "        \"\"\").save()\n",
    "        \n",
    "        notes_paras_df = notes_base_df.select(\n",
    "            col(\"explanatory_notes.uri\").alias(\"notes_id\"),\n",
    "            col(\"legis_uri\"),\n",
    "            explode_outer(\"explanatory_notes.paragraphs\").alias(\"paragraph\")\n",
    "        ).filter(col(\"paragraph\").isNotNull()) \\\n",
    "         .withColumn(\"para_id\", concat(col(\"notes_id\"), lit(\"#enp_\"), md5(col(\"paragraph.text\").cast(\"string\"))))\n",
    "         \n",
    "        notes_paras_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (en:ExplanatoryNotes {id: row.notes_id})\n",
    "            MERGE (p:ExplanatoryNotesParagraph {id: row.para_id})\n",
    "            SET p.text = row.`paragraph.text`,\n",
    "                p.uri = row.`paragraph.uri`\n",
    "            MERGE (en)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\").save()\n",
    "        \n",
    "        return notes_paras_df\n",
    "\n",
    "    def _write_explanatory_notes_citations(self, notes_paras_df):\n",
    "        if notes_paras_df is None:\n",
    "            return\n",
    "            \n",
    "        comm_fields = notes_paras_df.schema[\"paragraph\"].dataType.fieldNames()\n",
    "        if \"citations\" not in comm_fields:\n",
    "            return\n",
    "            \n",
    "        print(\"Writing Explanatory Notes Citation Nodes... (Sequential & Strict Match)\")\n",
    "        citations_df = notes_paras_df.select(\n",
    "            col(\"para_id\"),\n",
    "            explode_outer(\"paragraph.citations\").alias(\"citation\")\n",
    "        ).filter(col(\"citation\").isNotNull())\n",
    "        \n",
    "        cit_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "        \n",
    "        citations_flat = citations_df.select(\n",
    "            col(\"para_id\"),\n",
    "            col(\"citation.id\").alias(\"cit_id\") if \"id\" in cit_fields else lit(None).alias(\"cit_id\"),\n",
    "            col(\"citation.uri\").alias(\"cit_uri\") if \"uri\" in cit_fields else lit(None).alias(\"cit_uri\"),\n",
    "            col(\"citation.title\").alias(\"cit_title\") if \"title\" in cit_fields else lit(None).alias(\"cit_title\"),\n",
    "            col(\"citation.year\").alias(\"cit_year\") if \"year\" in cit_fields else lit(None).alias(\"cit_year\"),\n",
    "            col(\"citation.class\").alias(\"cit_class\") if \"class\" in cit_fields else lit(None).alias(\"cit_class\"),\n",
    "            col(\"citation.text\").alias(\"cit_text\") if \"text\" in cit_fields else lit(None).alias(\"cit_text\")\n",
    "        ).filter(col(\"cit_id\").isNotNull()).dropDuplicates([\"para_id\", \"cit_id\"])\n",
    "\n",
    "        citations_flat = citations_flat.withColumn(\"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\"))\n",
    "        citations_flat = citations_flat.coalesce(1)\n",
    "\n",
    "        citations_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.para_id IS NOT NULL AND row.cit_id IS NOT NULL\n",
    "            \n",
    "            MATCH (p:ExplanatoryNotesParagraph {id: row.para_id})\n",
    "            MERGE (cit:Citation {id: row.cit_id})\n",
    "            SET cit.uri = row.cit_uri,\n",
    "                cit.title = row.cit_title,\n",
    "                cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class,\n",
    "                cit.text = row.cit_text\n",
    "            MERGE (p)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            WITH cit, row\n",
    "            WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\").save()\n",
    "\n",
    "    def load_full_hierarchy_to_neo4j(self, json_dir=None):\n",
    "        if json_dir is None:\n",
    "            json_dir = f\"{self.json_output_dir}/*/*.json\"\n",
    "            \n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Legislation Full Graph Builder\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.2_for_spark_3\") \\\n",
    "            .config(\"neo4j.url\", self.uri) \\\n",
    "            .config(\"neo4j.authentication.basic.username\", self.user) \\\n",
    "            .config(\"neo4j.authentication.basic.password\", self.password) \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "        raw_df = spark.read \\\n",
    "            .option(\"multiline\", \"true\") \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "            .option(\"recursiveFileLookup\", \"true\") \\\n",
    "            .option(\"pathGlobFilter\", \"*.json\") \\\n",
    "            .json(json_dir)\n",
    "\n",
    "        if \"_corrupt_record\" in raw_df.columns:\n",
    "            raw_df = raw_df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "            \n",
    "        raw_df = raw_df.filter(col(\"legislation_url\").isNotNull() & (col(\"legislation_url\") != \"\"))\n",
    "        \n",
    "        self._write_legislation_nodes(raw_df)\n",
    "        self._write_super_relationships(raw_df)\n",
    "        parts_df = self._write_part_nodes(raw_df)\n",
    "        chapters_df = self._write_chapter_nodes(parts_df)\n",
    "        sections_df = self._write_section_nodes(chapters_df)\n",
    "        paragraphs_df = self._write_paragraph_nodes(sections_df)\n",
    "        \n",
    "        sched_paras_df, sched_para_comm_df, sched_subpara_comm_df = self._write_schedules_nodes(raw_df)\n",
    "        notes_paras_df = self._write_explanatory_notes_nodes(raw_df)\n",
    "        self._write_explanatory_notes_citations(notes_paras_df)\n",
    "\n",
    "        sec_comm_df = sections_df.select(col(\"sec_id\").alias(\"parent_id\"), explode_outer(\"section.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "        para_comm_df = paragraphs_df.select(col(\"para_id\").alias(\"parent_id\"), explode_outer(\"paragraph.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        self._write_commentaries(sec_comm_df, para_comm_df, sched_para_comm_df, sched_subpara_comm_df)\n",
    "\n",
    "        all_comms = sec_comm_df.select(\"commentary\").unionByName(para_comm_df.select(\"commentary\"), allowMissingColumns=True)\n",
    "        if sched_para_comm_df is not None:\n",
    "            all_comms = all_comms.unionByName(sched_para_comm_df.select(\"commentary\"), allowMissingColumns=True)\n",
    "        if sched_subpara_comm_df is not None:\n",
    "            all_comms = all_comms.unionByName(sched_subpara_comm_df.select(\"commentary\"), allowMissingColumns=True)\n",
    "\n",
    "        self._write_citations(all_comms)\n",
    "        self._write_citation_subrefs(all_comms)\n",
    "\n",
    "        print(\"Graph load complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5af65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def setup_neo4j_constraints(uri, user, password, database):\n",
    "    \"\"\"\n",
    "    Connects directly to Neo4j to ensure unique constraints and indexes exist \n",
    "    before Spark starts pushing data. This prevents duplicate nodes and makes MERGE fast.\n",
    "    \"\"\"\n",
    "    print(\"Setting up Neo4j constraints...\")\n",
    "    constraints = [\n",
    "        \"CREATE CONSTRAINT leg_uri_unique IF NOT EXISTS FOR (l:Legislation) REQUIRE l.uri IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT part_id_unique IF NOT EXISTS FOR (p:Part) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT chap_id_unique IF NOT EXISTS FOR (c:Chapter) REQUIRE c.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sec_id_unique IF NOT EXISTS FOR (s:Section) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT para_id_unique IF NOT EXISTS FOR (pa:Paragraph) REQUIRE pa.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_id_unique IF NOT EXISTS FOR (s:Schedule) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_para_id_unique IF NOT EXISTS FOR (p:ScheduleParagraph) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_subpara_id_unique IF NOT EXISTS FOR (sp:ScheduleSubparagraph) REQUIRE sp.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT com_id_unique IF NOT EXISTS FOR (com:Commentary) REQUIRE com.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT cit_id_unique IF NOT EXISTS FOR (cit:Citation) REQUIRE cit.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sub_id_unique IF NOT EXISTS FOR (sub:CitationSubRef) REQUIRE sub.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT en_id_unique IF NOT EXISTS FOR (en:ExplanatoryNotes) REQUIRE en.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT ep_id_unique IF NOT EXISTS FOR (ep:ExplanatoryNotesParagraph) REQUIRE ep.id IS UNIQUE;\"\n",
    "    ]\n",
    "    \n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session(database=database) as session:\n",
    "        for query in constraints:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "    print(\"Constraints successfully applied.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5e23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neo4j constraints...\n",
      "Constraints successfully applied.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 22:18:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Legislation Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Super Relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Part Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Chapter Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Section Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Sub-paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Explanatory Notes Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Explanatory Notes Citation Nodes... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Commentary Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation Nodes... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 22:24:04 WARN DAGScheduler: Broadcasting large task binary with size 1379.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation SubRefs... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 22:24:42 WARN DAGScheduler: Broadcasting large task binary with size 1375.9 KiB\n",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph load complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "setup_neo4j_constraints(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE)\n",
    "loader = LegislationGraphLoader(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, JSON_OUTPUT_DIR)\n",
    "loader.load_full_hierarchy_to_neo4j()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-legislation-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
