{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2093b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LEGISLATION_URL_PREFIX = os.getenv('LEGISLATION_URL_PREFIX')\n",
    "LEGISLATION_URI_LIST_FILE = os.getenv('LEGISLATION_URI_LIST_FILE')\n",
    "JSON_OUTPUT_DIR = os.getenv('JSON_OUTPUT_DIR', 'json_out')\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d08ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.user\n",
      "Warning: Ignoring non-Spark config property: neo4j.database\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "26/02/21 13:36:36 WARN Utils: Your hostname, Pedros-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.181 instead (on interface en0)\n",
      "26/02/21 13:36:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/pedroleitao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pedroleitao/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-066e6650-4697-4d95-984d-24c33e30a245;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pedroleitao/miniconda3/envs/legal-legislation-explorer/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#caniuse-core;1.3.0 in central\n",
      "\tfound org.neo4j#caniuse-api;1.3.0 in central\n",
      "\tfound org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in central\n",
      "\tfound org.jetbrains#annotations;13.0 in central\n",
      "\tfound org.neo4j#caniuse-neo4j-detection;1.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound io.netty#netty-handler;4.1.127.Final in central\n",
      "\tfound io.netty#netty-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.127.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.127.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.73.Final in central\n",
      "\tfound io.projectreactor#reactor-core;3.6.11 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in central\n",
      "\tfound org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/neo4j-connector-apache-spark_2.12/5.3.10_for_spark_3/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3!neo4j-connector-apache-spark_2.12.jar (373ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/neo4j-connector-apache-spark_2.12_common/5.3.10_for_spark_3/neo4j-connector-apache-spark_2.12_common-5.3.10_for_spark_3.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3!neo4j-connector-apache-spark_2.12_common.jar (82ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/driver/neo4j-java-driver-slim/4.4.21/neo4j-java-driver-slim-4.4.21.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j.driver#neo4j-java-driver-slim;4.4.21!neo4j-java-driver-slim.jar (89ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/caniuse-core/1.3.0/caniuse-core-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#caniuse-core;1.3.0!caniuse-core.jar (80ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/caniuse-neo4j-detection/1.3.0/caniuse-neo4j-detection-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#caniuse-neo4j-detection;1.3.0!caniuse-neo4j-detection.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/neo4j-cypher-dsl/2022.11.0/neo4j-cypher-dsl-2022.11.0.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#neo4j-cypher-dsl;2022.11.0!neo4j-cypher-dsl.jar (232ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/connectors/commons-authn-spi/1.0.0-rc2/commons-authn-spi-1.0.0-rc2.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j.connectors#commons-authn-spi;1.0.0-rc2!commons-authn-spi.jar (87ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/connectors/commons-reauth-driver/1.0.0-rc2/commons-reauth-driver-1.0.0-rc2.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2!commons-reauth-driver.jar (77ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/caniuse-api/1.3.0/caniuse-api-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#caniuse-api;1.3.0!caniuse-api.jar (75ms)\n",
      "downloading https://repo1.maven.org/maven2/org/jetbrains/kotlin/kotlin-stdlib/2.1.20/kotlin-stdlib-2.1.20.jar ...\n",
      "\t[SUCCESSFUL ] org.jetbrains.kotlin#kotlin-stdlib;2.1.20!kotlin-stdlib.jar (135ms)\n",
      "downloading https://repo1.maven.org/maven2/org/jetbrains/annotations/13.0/annotations-13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.jetbrains#annotations;13.0!annotations.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.4!reactive-streams.jar (86ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-handler/4.1.127.Final/netty-handler-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-handler;4.1.127.Final!netty-handler.jar (93ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-tcnative-classes/2.0.73.Final/netty-tcnative-classes-2.0.73.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-tcnative-classes;2.0.73.Final!netty-tcnative-classes.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/io/projectreactor/reactor-core/3.6.11/reactor-core-3.6.11.jar ...\n",
      "\t[SUCCESSFUL ] io.projectreactor#reactor-core;3.6.11!reactor-core.jar (119ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-common/4.1.127.Final/netty-common-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-common;4.1.127.Final!netty-common.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-resolver/4.1.127.Final/netty-resolver-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-resolver;4.1.127.Final!netty-resolver.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-buffer/4.1.127.Final/netty-buffer-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-buffer;4.1.127.Final!netty-buffer.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport/4.1.127.Final/netty-transport-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport;4.1.127.Final!netty-transport.jar (94ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport-native-unix-common/4.1.127.Final/netty-transport-native-unix-common-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport-native-unix-common;4.1.127.Final!netty-transport-native-unix-common.jar (78ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec/4.1.127.Final/netty-codec-4.1.127.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec;4.1.127.Final!netty-codec.jar (120ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apiguardian#apiguardian-api;1.1.2!apiguardian-api.jar (92ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.17/slf4j-api-2.0.17.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.17!slf4j-api.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/connectors/commons-authn-provided/1.0.0-rc2/commons-authn-provided-1.0.0-rc2.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j.connectors#commons-authn-provided;1.0.0-rc2!commons-authn-provided.jar (88ms)\n",
      ":: resolution report :: resolve 23143ms :: artifacts dl 2569ms\n",
      "\t:: modules in use:\n",
      "\tio.netty#netty-buffer;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.73.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.127.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.6.11 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.jetbrains#annotations;13.0 from central in [default]\n",
      "\torg.jetbrains.kotlin#kotlin-stdlib;2.1.20 from central in [default]\n",
      "\torg.neo4j#caniuse-api;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-core;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-neo4j-detection;1.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver-slim;4.4.21 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   24  |   24  |   0   ||   24  |   24  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-066e6650-4697-4d95-984d-24c33e30a245\n",
      "\tconfs: [default]\n",
      "\t24 artifacts copied, 0 already retrieved (15367kB/19ms)\n",
      "26/02/21 13:37:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Scala version: 5\n",
      "Neo4j Connector version: 5.3.10_for_spark_3\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark with Neo4j Connector\n",
    "neo4j_maven_pkg = \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.10_for_spark_3\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PSC_Loader_Spark\")\n",
    "    .config(\"spark.jars.packages\", neo4j_maven_pkg)\n",
    "    .config(\"neo4j.url\", NEO4J_URI)\n",
    "    .config(\"neo4j.authentication.basic.user\", NEO4J_USER)\n",
    "    .config(\"neo4j.authentication.basic.password\", NEO4J_PASSWORD)\n",
    "    .config(\"neo4j.database\", NEO4J_DATABASE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Check Spark and Connector versions\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Scala version: {spark.sparkContext.version.split('.')[1]}\")\n",
    "print(f\"Neo4j Connector version: {neo4j_maven_pkg.split(':')[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bbe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode_outer, concat, lit, coalesce, md5, to_date, regexp_replace\n",
    "\n",
    "# PySpark Graph Builder\n",
    "def load_full_hierarchy_to_neo4j(json_dir=f\"{JSON_OUTPUT_DIR}/*.json\"):\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Legislation Full Graph Builder\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.2_for_spark_3\") \\\n",
    "        .config(\"neo4j.url\", NEO4J_URI) \\\n",
    "        .config(\"neo4j.authentication.basic.username\", NEO4J_USER) \\\n",
    "        .config(\"neo4j.authentication.basic.password\", NEO4J_PASSWORD) \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    raw_df = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .option(\"mode\", \"PERMISSIVE\") \\\n",
    "        .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "        .option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .option(\"pathGlobFilter\", \"*.json\") \\\n",
    "        .json(json_dir)\n",
    "\n",
    "    if \"_corrupt_record\" in raw_df.columns:\n",
    "        raw_df = raw_df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "        \n",
    "    # Filter out any legislation without a valid URL (since this is our primary key for linking)\n",
    "    raw_df = raw_df.filter(col(\"legislation_url\").isNotNull() & (col(\"legislation_url\") != \"\"))\n",
    "    \n",
    "    # Root Legislation Nodes\n",
    "    print(\"Writing Legislation Nodes...\")\n",
    "    select_exprs = [\n",
    "        col(\"legislation_url\").alias(\"uri\"),\n",
    "        col(\"identifier.title\").alias(\"title\"),\n",
    "        col(\"identifier.description\").alias(\"description\")\n",
    "    ]\n",
    "    \n",
    "    if \"identifier\" in raw_df.columns and \"modified\" in raw_df.schema[\"identifier\"].dataType.fieldNames():\n",
    "        select_exprs.append(to_date(col(\"identifier.modified\"), \"yyyy-MM-dd\").alias(\"modified_date\"))\n",
    "    else:\n",
    "        select_exprs.append(lit(None).cast(\"date\").alias(\"modified_date\"))\n",
    "        \n",
    "    if \"identifier\" in raw_df.columns and \"valid_date\" in raw_df.schema[\"identifier\"].dataType.fieldNames():\n",
    "        select_exprs.append(to_date(col(\"identifier.valid_date\"), \"yyyy-MM-dd\").alias(\"valid_date\"))\n",
    "    else:\n",
    "        select_exprs.append(lit(None).cast(\"date\").alias(\"valid_date\"))\n",
    "\n",
    "    if \"metadata\" in raw_df.columns and \"enactment_date\" in raw_df.schema[\"metadata\"].dataType.fieldNames():\n",
    "        select_exprs.append(to_date(col(\"metadata.enactment_date\"), \"yyyy-MM-dd\").alias(\"enactment_date\"))\n",
    "    else:\n",
    "        select_exprs.append(lit(None).cast(\"date\").alias(\"enactment_date\"))\n",
    "\n",
    "    legis_df = raw_df.select(*select_exprs).dropDuplicates([\"uri\"])\n",
    "\n",
    "    legis_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MERGE (l:Legislation {uri: row.uri})\n",
    "        SET l.title = row.title, \n",
    "            l.description = row.description,\n",
    "            l.modified_date = row.modified_date,\n",
    "            l.valid_date = row.valid_date,\n",
    "            l.enactment_date = row.enactment_date\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Main Body Hierarchy\n",
    "    print(\"Writing Part Nodes...\")\n",
    "    parts_df = raw_df.select(\n",
    "        col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "        explode_outer(\"parts\").alias(\"part\")\n",
    "    ).filter(col(\"part\").isNotNull()) \\\n",
    "     .withColumn(\"part_num\", col(\"part.part_number\")) \\\n",
    "     .withColumn(\"part_id\", concat(col(\"legis_uri\"), lit(\"#part_\"), coalesce(col(\"part_num\"), md5(col(\"part\").cast(\"string\")))))\n",
    "\n",
    "    parts_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (l:Legislation {uri: row.legis_uri})\n",
    "        MERGE (p:Part {id: row.part_id})\n",
    "        SET p.number = row.part_num, p.title = row.part.title\n",
    "        MERGE (l)-[:HAS_PART]->(p)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Chapter Nodes\n",
    "    print(\"Writing Chapter Nodes...\")\n",
    "    chapters_df = parts_df.select(\n",
    "        col(\"part_id\"),\n",
    "        explode_outer(\"part.chapters\").alias(\"chapter\")\n",
    "    ).filter(col(\"chapter\").isNotNull()) \\\n",
    "     .withColumn(\"chapter_num\", col(\"chapter.chapter_number\")) \\\n",
    "     .withColumn(\"chapter_uri\", col(\"chapter.uri\")) \\\n",
    "     .withColumn(\"chapter_id\", coalesce(col(\"chapter_uri\"), concat(col(\"part_id\"), lit(\"#chapter_\"), coalesce(col(\"chapter_num\"), md5(col(\"chapter\").cast(\"string\"))))))\n",
    "\n",
    "    chapters_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (p:Part {id: row.part_id})\n",
    "        MERGE (c:Chapter {id: row.chapter_id})\n",
    "        SET c.number = row.chapter_num, \n",
    "            c.title = row.chapter.title,\n",
    "            c.uri = row.chapter_uri\n",
    "        MERGE (p)-[:HAS_CHAPTER]->(c)\n",
    "    \"\"\").save()\n",
    "\n",
    "    print(\"Writing Section Nodes...\")\n",
    "    sections_df = chapters_df.select(\n",
    "        col(\"chapter_id\"),\n",
    "        explode_outer(\"chapter.sections\").alias(\"section\")\n",
    "    ).filter(col(\"section\").isNotNull()) \\\n",
    "     .withColumn(\"sec_id\", coalesce(col(\"section.uri\"), concat(col(\"chapter_id\"), lit(\"#sec_\"), coalesce(col(\"section.section_number\"), md5(col(\"section\").cast(\"string\"))))))\n",
    "    \n",
    "    sections_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (c:Chapter {id: row.chapter_id})\n",
    "        MERGE (s:Section {id: row.sec_id})\n",
    "        SET s.number = row.section.section_number, \n",
    "            s.title = row.section.title, \n",
    "            s.uri = row.section.uri\n",
    "        MERGE (c)-[:HAS_SECTION]->(s)\n",
    "    \"\"\").save()\n",
    "\n",
    "    print(\"Writing Paragraph Nodes...\")\n",
    "    paragraphs_df = sections_df.select(\n",
    "        col(\"sec_id\"),\n",
    "        explode_outer(\"section.paragraphs\").alias(\"paragraph\")\n",
    "    ).filter(col(\"paragraph\").isNotNull()) \\\n",
    "     .withColumn(\"para_id\", coalesce(col(\"paragraph.uri\"), concat(col(\"sec_id\"), lit(\"#para_\"), coalesce(col(\"paragraph.paragraph_number\"), md5(col(\"paragraph\").cast(\"string\"))))))\n",
    "\n",
    "    paragraphs_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "        UNWIND event AS row\n",
    "        MATCH (s:Section {id: row.sec_id})\n",
    "        MERGE (pa:Paragraph {id: row.para_id})\n",
    "        SET pa.number = row.paragraph.paragraph_number, \n",
    "            pa.text = row.paragraph.text, \n",
    "            pa.uri = row.paragraph.uri\n",
    "        MERGE (s)-[:HAS_PARAGRAPH]->(pa)\n",
    "    \"\"\").save()\n",
    "\n",
    "    # Schedules Hierarchy\n",
    "    sched_para_comm_df = None\n",
    "    sched_subpara_comm_df = None\n",
    "\n",
    "    if \"schedules\" in raw_df.columns:\n",
    "        print(\"Writing Schedule Nodes...\")\n",
    "        schedules_df = raw_df.select(\n",
    "            col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "            explode_outer(\"schedules\").alias(\"schedule\")\n",
    "        ).filter(col(\"schedule\").isNotNull()) \\\n",
    "         .withColumn(\"sched_id\", coalesce(col(\"schedule.uri\"), concat(col(\"legis_uri\"), lit(\"#sched_\"), coalesce(col(\"schedule.schedule_number\"), md5(col(\"schedule\").cast(\"string\"))))))\n",
    "         \n",
    "        schedules_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (sc:Schedule {id: row.sched_id})\n",
    "            SET sc.number = row.schedule.schedule_number,\n",
    "                sc.title = row.schedule.title,\n",
    "                sc.reference = row.schedule.reference,\n",
    "                sc.uri = row.schedule.uri\n",
    "            MERGE (l)-[:HAS_SCHEDULE]->(sc)\n",
    "        \"\"\").save()\n",
    "\n",
    "        print(\"Writing Schedule Paragraph Nodes...\")\n",
    "        sched_paras_df = schedules_df.select(\n",
    "            col(\"sched_id\"),\n",
    "            explode_outer(\"schedule.paragraphs\").alias(\"paragraph\")\n",
    "        ).filter(col(\"paragraph\").isNotNull()) \\\n",
    "         .withColumn(\"para_id\", coalesce(col(\"paragraph.uri\"), concat(col(\"sched_id\"), lit(\"#spara_\"), coalesce(col(\"paragraph.paragraph_number\"), md5(col(\"paragraph\").cast(\"string\"))))))\n",
    "\n",
    "        sched_paras_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (sc:Schedule {id: row.sched_id})\n",
    "            MERGE (p:ScheduleParagraph {id: row.para_id})\n",
    "            SET p.number = row.paragraph.paragraph_number,\n",
    "                p.crossheading = row.paragraph.crossheading,\n",
    "                p.text = row.paragraph.text,\n",
    "                p.uri = row.paragraph.uri\n",
    "            MERGE (sc)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\").save()\n",
    "\n",
    "        sched_para_comm_df = sched_paras_df.select(col(\"para_id\").alias(\"parent_id\"), explode_outer(\"paragraph.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        if \"subparagraphs\" in sched_paras_df.schema[\"paragraph\"].dataType.fieldNames():\n",
    "            print(\"Writing Schedule Sub-paragraph Nodes...\")\n",
    "            sched_subparas_df = sched_paras_df.select(\n",
    "                col(\"para_id\"),\n",
    "                explode_outer(\"paragraph.subparagraphs\").alias(\"subparagraph\")\n",
    "            ).filter(col(\"subparagraph\").isNotNull()) \\\n",
    "             .withColumn(\"subpara_id\", coalesce(col(\"subparagraph.uri\"), concat(col(\"para_id\"), lit(\"#ssub_\"), coalesce(col(\"subparagraph.subparagraph_number\"), md5(col(\"subparagraph\").cast(\"string\"))))))\n",
    "\n",
    "            sched_subparas_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "                UNWIND event AS row\n",
    "                MATCH (p:ScheduleParagraph {id: row.para_id})\n",
    "                MERGE (sp:ScheduleSubparagraph {id: row.subpara_id})\n",
    "                SET sp.number = row.subparagraph.subparagraph_number,\n",
    "                    sp.text = row.subparagraph.text,\n",
    "                    sp.uri = row.subparagraph.uri\n",
    "                MERGE (p)-[:HAS_SUBPARAGRAPH]->(sp)\n",
    "            \"\"\").save()\n",
    "\n",
    "            sched_subpara_comm_df = sched_subparas_df.select(col(\"subpara_id\").alias(\"parent_id\"), explode_outer(\"subparagraph.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "    # Commentary Extraction & Linking\n",
    "    print(\"Writing Commentary Nodes...\")\n",
    "    \n",
    "    sec_comm_df = sections_df.select(col(\"sec_id\").alias(\"parent_id\"), explode_outer(\"section.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "    para_comm_df = paragraphs_df.select(col(\"para_id\").alias(\"parent_id\"), explode_outer(\"paragraph.commentaries\").alias(\"commentary\")).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "    # Safely flatten commentary structs before writing to Neo4j\n",
    "    def write_commentaries(df, parent_label):\n",
    "        if df is not None:\n",
    "            fields = df.schema[\"commentary\"].dataType.fieldNames()\n",
    "            type_col = col(\"commentary.type\") if \"type\" in fields else lit(None)\n",
    "            text_col = col(\"commentary.text\") if \"text\" in fields else lit(None)\n",
    "            \n",
    "            flat_df = df.select(\n",
    "                col(\"parent_id\"),\n",
    "                col(\"commentary.ref_id\").alias(\"ref_id\"),\n",
    "                type_col.alias(\"type\"),\n",
    "                text_col.alias(\"text\")\n",
    "            ).filter(col(\"ref_id\").isNotNull()).dropDuplicates([\"parent_id\", \"ref_id\"])\n",
    "            \n",
    "            flat_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", f\"\"\"\n",
    "                UNWIND event AS row\n",
    "                WITH row WHERE row.ref_id IS NOT NULL\n",
    "                MATCH (parent:{parent_label} {{id: row.parent_id}})\n",
    "                MERGE (com:Commentary {{id: row.ref_id}})\n",
    "                SET com.type = row.type, com.text = row.text\n",
    "                MERGE (parent)-[:HAS_COMMENTARY]->(com)\n",
    "            \"\"\").save()\n",
    "\n",
    "    write_commentaries(sec_comm_df, \"Section\")\n",
    "    write_commentaries(para_comm_df, \"Paragraph\")\n",
    "    write_commentaries(sched_para_comm_df, \"ScheduleParagraph\")\n",
    "    write_commentaries(sched_subpara_comm_df, \"ScheduleSubparagraph\")\n",
    "\n",
    "    # Citations and Cross-Links (Fixed Topology & URIs)\n",
    "    all_comms = sec_comm_df.select(\"commentary\").unionByName(para_comm_df.select(\"commentary\"), allowMissingColumns=True)\n",
    "    if sched_para_comm_df is not None:\n",
    "        all_comms = all_comms.unionByName(sched_para_comm_df.select(\"commentary\"), allowMissingColumns=True)\n",
    "    if sched_subpara_comm_df is not None:\n",
    "        all_comms = all_comms.unionByName(sched_subpara_comm_df.select(\"commentary\"), allowMissingColumns=True)\n",
    "\n",
    "    comm_fields = all_comms.schema[\"commentary\"].dataType.fieldNames()\n",
    "    \n",
    "    if \"citations\" in comm_fields:\n",
    "        print(\"Writing Citation Nodes... (Sequential & Strict Match)\")\n",
    "        citations_df = all_comms.select(\n",
    "            col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "            explode_outer(\"commentary.citations\").alias(\"citation\")\n",
    "        ).filter(col(\"citation\").isNotNull())\n",
    "        \n",
    "        cit_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "        \n",
    "        citations_flat = citations_df.select(\n",
    "            col(\"comm_id\"),\n",
    "            col(\"citation.id\").alias(\"cit_id\") if \"id\" in cit_fields else lit(None).alias(\"cit_id\"),\n",
    "            col(\"citation.uri\").alias(\"cit_uri\") if \"uri\" in cit_fields else lit(None).alias(\"cit_uri\"),\n",
    "            col(\"citation.title\").alias(\"cit_title\") if \"title\" in cit_fields else lit(None).alias(\"cit_title\"),\n",
    "            col(\"citation.year\").alias(\"cit_year\") if \"year\" in cit_fields else lit(None).alias(\"cit_year\"),\n",
    "            col(\"citation.class\").alias(\"cit_class\") if \"class\" in cit_fields else lit(None).alias(\"cit_class\"),\n",
    "            col(\"citation.text\").alias(\"cit_text\") if \"text\" in cit_fields else lit(None).alias(\"cit_text\")\n",
    "        ).filter(col(\"cit_id\").isNotNull()).dropDuplicates([\"comm_id\", \"cit_id\"])\n",
    "\n",
    "        # Strip '/id/' from the URI so it matches our root Legislation nodes perfectly\n",
    "        citations_flat = citations_flat.withColumn(\"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\"))\n",
    "        citations_flat = citations_flat.coalesce(1)\n",
    "\n",
    "        citations_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL AND row.cit_id IS NOT NULL\n",
    "            \n",
    "            // Create the Citation Node and link it to the Commentary\n",
    "            MATCH (com:Commentary {id: row.comm_id})\n",
    "            MERGE (cit:Citation {id: row.cit_id})\n",
    "            SET cit.uri = row.cit_uri,\n",
    "                cit.title = row.cit_title,\n",
    "                cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class,\n",
    "                cit.text = row.cit_text\n",
    "            MERGE (com)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            // Strict Match: Only draw CITES_ACT if the target Legislation actually exists\n",
    "            WITH cit, row\n",
    "            WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\").save()\n",
    "\n",
    "    if \"citation_subrefs\" in comm_fields:\n",
    "        print(\"Writing Citation SubRefs... (Sequential & Strict Match)\")\n",
    "        subrefs_df = all_comms.select(\n",
    "            col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "            explode_outer(\"commentary.citation_subrefs\").alias(\"subref\")\n",
    "        ).filter(col(\"subref\").isNotNull())\n",
    "        \n",
    "        sub_fields = subrefs_df.schema[\"subref\"].dataType.fieldNames()\n",
    "\n",
    "        subrefs_flat = subrefs_df.select(\n",
    "            col(\"comm_id\"),\n",
    "            col(\"subref.id\").alias(\"sub_id\") if \"id\" in sub_fields else lit(None).alias(\"sub_id\"),\n",
    "            col(\"subref.citation_ref\").alias(\"citation_ref\") if \"citation_ref\" in sub_fields else lit(None).alias(\"citation_ref\"),\n",
    "            col(\"subref.uri\").alias(\"sub_uri\") if \"uri\" in sub_fields else lit(None).alias(\"sub_uri\"),\n",
    "            col(\"subref.section_ref\").alias(\"sub_section_ref\") if \"section_ref\" in sub_fields else lit(None).alias(\"sub_section_ref\"),\n",
    "            col(\"subref.text\").alias(\"sub_text\") if \"text\" in sub_fields else lit(None).alias(\"sub_text\")\n",
    "        ).filter(col(\"sub_id\").isNotNull()).dropDuplicates([\"comm_id\", \"sub_id\"])\n",
    "\n",
    "        # Extract the Base URI from the deep SubRef URI (e.g., .../ukpga/2009/26/section/55/1 -> .../ukpga/2009/26)\n",
    "        # This allows us to map the subref directly to the parent Act node\n",
    "        subrefs_flat = subrefs_flat.withColumn(\"base_uri\", \n",
    "            regexp_replace(col(\"sub_uri\"), r\"(http://www\\.legislation\\.gov\\.uk)/id/([^/]+/[0-9]+/[0-9]+).*\", \"$1/$2\")\n",
    "        )\n",
    "        subrefs_flat = subrefs_flat.coalesce(1)\n",
    "\n",
    "        subrefs_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\"query\", \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL AND row.sub_id IS NOT NULL\n",
    "            \n",
    "            // Create the SubRef node\n",
    "            MERGE (sub:CitationSubRef {id: row.sub_id})\n",
    "            SET sub.uri = row.sub_uri, \n",
    "                sub.section_ref = row.sub_section_ref, \n",
    "                sub.text = row.sub_text\n",
    "                \n",
    "            // Conditional Link: Link to Citation if citation_ref exists, otherwise link to Commentary\n",
    "            WITH sub, row\n",
    "            MATCH (com:Commentary {id: row.comm_id})\n",
    "            OPTIONAL MATCH (cit:Citation {id: row.citation_ref})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN cit IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (cit)-[:HAS_SUBREF]->(sub)\n",
    "            )\n",
    "            FOREACH (_ IN CASE WHEN cit IS NULL THEN [1] ELSE [] END |\n",
    "                MERGE (com)-[:HAS_SUBREF]->(sub)\n",
    "            )\n",
    "            \n",
    "            // Strict Match: Link SubRef to the base Legislation Act if it exists\n",
    "            WITH sub, row\n",
    "            WHERE row.base_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.base_uri})\n",
    "            MERGE (sub)-[:REFERENCES]->(leg)\n",
    "        \"\"\").save()\n",
    "\n",
    "    print(\"Graph load complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5af65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def setup_neo4j_constraints(uri, user, password, database):\n",
    "    \"\"\"\n",
    "    Connects directly to Neo4j to ensure unique constraints and indexes exist \n",
    "    before Spark starts pushing data. This prevents duplicate nodes and makes MERGE fast.\n",
    "    \"\"\"\n",
    "    print(\"Setting up Neo4j constraints...\")\n",
    "    constraints = [\n",
    "        \"CREATE CONSTRAINT leg_uri_unique IF NOT EXISTS FOR (l:Legislation) REQUIRE l.uri IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT part_id_unique IF NOT EXISTS FOR (p:Part) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT chap_id_unique IF NOT EXISTS FOR (c:Chapter) REQUIRE c.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sec_id_unique IF NOT EXISTS FOR (s:Section) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT para_id_unique IF NOT EXISTS FOR (pa:Paragraph) REQUIRE pa.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_id_unique IF NOT EXISTS FOR (s:Schedule) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_para_id_unique IF NOT EXISTS FOR (p:ScheduleParagraph) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_subpara_id_unique IF NOT EXISTS FOR (sp:ScheduleSubparagraph) REQUIRE sp.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT com_id_unique IF NOT EXISTS FOR (com:Commentary) REQUIRE com.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT cit_id_unique IF NOT EXISTS FOR (cit:Citation) REQUIRE cit.id IS UNIQUE;\", # NEW\n",
    "        \"CREATE CONSTRAINT sub_id_unique IF NOT EXISTS FOR (sub:CitationSubRef) REQUIRE sub.id IS UNIQUE;\"\n",
    "    ]\n",
    "    \n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session(database=database) as session:\n",
    "        for query in constraints:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "    print(\"Constraints successfully applied.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5e23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neo4j constraints...\n",
      "Constraints successfully applied.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 13:37:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Legislation Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Part Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Chapter Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Section Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Sub-paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Commentary Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation Nodes... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 13:39:23 WARN DAGScheduler: Broadcasting large task binary with size 1350.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation SubRefs... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 13:39:36 WARN DAGScheduler: Broadcasting large task binary with size 1344.6 KiB\n",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph load complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "setup_neo4j_constraints(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE)\n",
    "load_full_hierarchy_to_neo4j(json_dir=JSON_OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-legislation-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
