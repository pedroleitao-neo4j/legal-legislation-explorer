{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2093b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LEGISLATION_URL_PREFIX = os.getenv(\"LEGISLATION_URL_PREFIX\")\n",
    "LEGISLATION_URI_LIST_FILE = os.getenv(\"LEGISLATION_URI_LIST_FILE\")\n",
    "JSON_OUTPUT_DIR = os.getenv(\"JSON_OUTPUT_DIR\", \"json_out\")\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d08ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.user\n",
      "Warning: Ignoring non-Spark config property: neo4j.database\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Ivy Default Cache set to: /Users/pedroleitao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pedroleitao/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bfc6b769-9224-4d41-a3df-cc4101e20073;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Volumes/Home/pedroleitao/miniconda3/envs/legal-legislation-explorer/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.neo4j#caniuse-core;1.3.0 in central\n",
      "\tfound org.neo4j#caniuse-api;1.3.0 in central\n",
      "\tfound org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in central\n",
      "\tfound org.jetbrains#annotations;13.0 in central\n",
      "\tfound org.neo4j#caniuse-neo4j-detection;1.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound io.netty#netty-handler;4.1.127.Final in central\n",
      "\tfound io.netty#netty-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.127.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.127.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.73.Final in central\n",
      "\tfound io.projectreactor#reactor-core;3.6.11 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in central\n",
      "\tfound org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in central\n",
      ":: resolution report :: resolve 238ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.netty#netty-buffer;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.73.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.127.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.6.11 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.jetbrains#annotations;13.0 from central in [default]\n",
      "\torg.jetbrains.kotlin#kotlin-stdlib;2.1.20 from central in [default]\n",
      "\torg.neo4j#caniuse-api;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-core;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-neo4j-detection;1.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver-slim;4.4.21 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   0   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bfc6b769-9224-4d41-a3df-cc4101e20073\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/4ms)\n",
      "26/02/26 14:39:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Scala version: 5\n",
      "Neo4j Connector version: 5.3.10_for_spark_3\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark with Neo4j Connector\n",
    "neo4j_maven_pkg = \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.10_for_spark_3\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PSC_Loader_Spark\")\n",
    "    .config(\"spark.jars.packages\", neo4j_maven_pkg)\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"neo4j.url\", NEO4J_URI)\n",
    "    .config(\"neo4j.authentication.basic.user\", NEO4J_USER)\n",
    "    .config(\"neo4j.authentication.basic.password\", NEO4J_PASSWORD)\n",
    "    .config(\"neo4j.database\", NEO4J_DATABASE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Check Spark and Connector versions\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Scala version: {spark.sparkContext.version.split('.')[1]}\")\n",
    "print(f\"Neo4j Connector version: {neo4j_maven_pkg.split(':')[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bbe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode_outer,\n",
    "    concat,\n",
    "    lit,\n",
    "    coalesce,\n",
    "    md5,\n",
    "    to_date,\n",
    "    regexp_replace,\n",
    ")\n",
    "\n",
    "\n",
    "class LegislationGraphLoader:\n",
    "    def __init__(self, uri, user, password, json_output_dir):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.json_output_dir = json_output_dir\n",
    "\n",
    "    def _write_legislation_nodes(self, raw_df):\n",
    "        print(\"Writing Legislation Nodes...\")\n",
    "        select_exprs = [\n",
    "            col(\"legislation_url\").alias(\"uri\"),\n",
    "            col(\"identifier.title\").alias(\"title\"),\n",
    "            col(\"identifier.description\").alias(\"description\"),\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            \"identifier\" in raw_df.columns\n",
    "            and \"modified\" in raw_df.schema[\"identifier\"].dataType.fieldNames()\n",
    "        ):\n",
    "            select_exprs.append(\n",
    "                to_date(col(\"identifier.modified\"), \"yyyy-MM-dd\").alias(\"modified_date\")\n",
    "            )\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"modified_date\"))\n",
    "\n",
    "        if (\n",
    "            \"identifier\" in raw_df.columns\n",
    "            and \"valid_date\" in raw_df.schema[\"identifier\"].dataType.fieldNames()\n",
    "        ):\n",
    "            select_exprs.append(\n",
    "                to_date(col(\"identifier.valid_date\"), \"yyyy-MM-dd\").alias(\"valid_date\")\n",
    "            )\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"valid_date\"))\n",
    "\n",
    "        if (\n",
    "            \"metadata\" in raw_df.columns\n",
    "            and \"enactment_date\" in raw_df.schema[\"metadata\"].dataType.fieldNames()\n",
    "        ):\n",
    "            select_exprs.append(\n",
    "                to_date(col(\"metadata.enactment_date\"), \"yyyy-MM-dd\").alias(\n",
    "                    \"enactment_date\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"enactment_date\"))\n",
    "\n",
    "        if (\n",
    "            \"metadata\" in raw_df.columns\n",
    "            and \"status\" in raw_df.schema[\"metadata\"].dataType.fieldNames()\n",
    "        ):\n",
    "            select_exprs.append(col(\"metadata.status\").alias(\"status\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).alias(\"status\"))\n",
    "\n",
    "        if (\n",
    "            \"metadata\" in raw_df.columns\n",
    "            and \"category\" in raw_df.schema[\"metadata\"].dataType.fieldNames()\n",
    "        ):\n",
    "            select_exprs.append(col(\"metadata.category\").alias(\"category\"))\n",
    "        else:\n",
    "            select_exprs.append(lit(None).alias(\"category\"))\n",
    "\n",
    "        if (\n",
    "            \"metadata\" in raw_df.columns\n",
    "            and \"coming_into_force\" in raw_df.schema[\"metadata\"].dataType.fieldNames()\n",
    "        ):\n",
    "            select_exprs.append(\n",
    "                to_date(col(\"metadata.coming_into_force\"), \"yyyy-MM-dd\").alias(\n",
    "                    \"coming_into_force\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            select_exprs.append(lit(None).cast(\"date\").alias(\"coming_into_force\"))\n",
    "\n",
    "        legis_df = raw_df.select(*select_exprs).dropDuplicates([\"uri\"])\n",
    "\n",
    "        legis_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MERGE (l:Legislation {uri: row.uri})\n",
    "            SET l.title = row.title, \n",
    "                l.description = row.description,\n",
    "                l.modified_date = row.modified_date,\n",
    "                l.valid_date = row.valid_date,\n",
    "                l.enactment_date = row.enactment_date,\n",
    "                l.status = row.status,\n",
    "                l.category = row.category,\n",
    "                l.coming_into_force = row.coming_into_force\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "    def _write_part_nodes(self, raw_df):\n",
    "        print(\"Writing Part Nodes...\")\n",
    "        parts_df = (\n",
    "            raw_df.select(\n",
    "                col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "                explode_outer(\"parts\").alias(\"part\"),\n",
    "            )\n",
    "            .filter(col(\"part\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"part_id\",\n",
    "                concat(\n",
    "                    col(\"legis_uri\"),\n",
    "                    lit(\"#part_\"),\n",
    "                    coalesce(col(\"part.part_number\"), md5(col(\"part\").cast(\"string\"))),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        parts_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (p:Part {id: row.part_id})\n",
    "            SET p.number = row.`part.part_number`,\n",
    "                p.order = row.`part.order`,\n",
    "                p.title = row.`part.title`,\n",
    "                p.uri = row.`part.uri`,\n",
    "                p.status = row.`part.status`,\n",
    "                p.restrict_start_date = row.`part.restrict_start_date`,\n",
    "                p.restrict_end_date = row.`part.restrict_end_date`\n",
    "            MERGE (l)-[:HAS_PART]->(p)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "        return parts_df\n",
    "\n",
    "    def _write_chapter_nodes(self, parts_df):\n",
    "        print(\"Writing Chapter Nodes...\")\n",
    "        chapters_df = (\n",
    "            parts_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"part_id\"),\n",
    "                explode_outer(\"part.chapters\").alias(\"chapter\"),\n",
    "            )\n",
    "            .filter(col(\"chapter\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"chapter_id\",\n",
    "                coalesce(\n",
    "                    col(\"chapter.uri\"),\n",
    "                    concat(\n",
    "                        col(\"part_id\"),\n",
    "                        lit(\"#chapter_\"),\n",
    "                        coalesce(\n",
    "                            col(\"chapter.chapter_number\"),\n",
    "                            md5(col(\"chapter\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        chapters_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (p:Part {id: row.part_id})\n",
    "            MERGE (c:Chapter {id: row.chapter_id})\n",
    "            SET c.number = row.`chapter.chapter_number`, \n",
    "                c.order = row.`chapter.order`,\n",
    "                c.title = row.`chapter.title`,\n",
    "                c.uri = row.`chapter.uri`,\n",
    "                c.status = row.`chapter.status`,\n",
    "                c.restrict_start_date = date(row.`chapter.restrict_start_date`),\n",
    "                c.restrict_end_date = date(row.`chapter.restrict_end_date`)\n",
    "            MERGE (p)-[:HAS_CHAPTER]->(c)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "        return chapters_df\n",
    "\n",
    "    def _write_section_nodes(self, chapters_df):\n",
    "        print(\"Writing Section Nodes...\")\n",
    "        sections_df = (\n",
    "            chapters_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"chapter_id\"),\n",
    "                explode_outer(\"chapter.sections\").alias(\"section\"),\n",
    "            )\n",
    "            .filter(col(\"section\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"sec_id\",\n",
    "                coalesce(\n",
    "                    col(\"section.uri\"),\n",
    "                    concat(\n",
    "                        col(\"chapter_id\"),\n",
    "                        lit(\"#sec_\"),\n",
    "                        coalesce(\n",
    "                            col(\"section.section_number\"),\n",
    "                            md5(col(\"section\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        sections_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (c:Chapter {id: row.chapter_id})\n",
    "            MERGE (s:Section {id: row.sec_id})\n",
    "            SET s.number = row.`section.section_number`, \n",
    "                s.order = row.`section.order`,\n",
    "                s.title = row.`section.title`, \n",
    "                s.uri = row.`section.uri`,\n",
    "                s.restrict_extent = row.`section.restrict_extent`,\n",
    "                s.restrict_start_date = date(row.`section.restrict_start_date`),\n",
    "                s.restrict_end_date = date(row.`section.restrict_end_date`)\n",
    "            MERGE (c)-[:HAS_SECTION]->(s)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "        return sections_df\n",
    "\n",
    "    def _write_paragraph_nodes(self, sections_df):\n",
    "        print(\"Writing Paragraph Nodes...\")\n",
    "        paragraphs_df = (\n",
    "            sections_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"sec_id\"),\n",
    "                explode_outer(\"section.paragraphs\").alias(\"paragraph\"),\n",
    "            )\n",
    "            .filter(col(\"paragraph\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"para_id\",\n",
    "                coalesce(\n",
    "                    col(\"paragraph.uri\"),\n",
    "                    concat(\n",
    "                        col(\"sec_id\"),\n",
    "                        lit(\"#para_\"),\n",
    "                        coalesce(\n",
    "                            col(\"paragraph.paragraph_number\"),\n",
    "                            md5(col(\"paragraph\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        paragraphs_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (s:Section {id: row.sec_id})\n",
    "            MERGE (pa:Paragraph {id: row.para_id})\n",
    "            SET pa.number = row.`paragraph.paragraph_number`,\n",
    "                pa.order = row.`paragraph.order`,\n",
    "                pa.text = row.`paragraph.text`, \n",
    "                pa.uri = row.`paragraph.uri`\n",
    "            MERGE (s)-[:HAS_PARAGRAPH]->(pa)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "        return paragraphs_df\n",
    "\n",
    "    def _write_schedules_nodes(self, raw_df):\n",
    "        if (\n",
    "            \"schedules\" not in raw_df.columns\n",
    "            or raw_df.schema[\"schedules\"].dataType.simpleString() == \"array<string>\"\n",
    "        ):\n",
    "            return None, None, None\n",
    "\n",
    "        print(\"Writing Schedule Nodes...\")\n",
    "        schedules_df = (\n",
    "            raw_df.select(\n",
    "                col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "                explode_outer(\"schedules\").alias(\"schedule\"),\n",
    "            )\n",
    "            .filter(col(\"schedule\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"sched_id\",\n",
    "                coalesce(\n",
    "                    col(\"schedule.uri\"),\n",
    "                    concat(\n",
    "                        col(\"legis_uri\"),\n",
    "                        lit(\"#sched_\"),\n",
    "                        coalesce(\n",
    "                            col(\"schedule.schedule_number\"),\n",
    "                            md5(col(\"schedule\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        schedules_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (sc:Schedule {id: row.sched_id})\n",
    "            SET sc.number = row.`schedule.schedule_number`,\n",
    "                sc.order = row.`schedule.order`,\n",
    "                sc.title = row.`schedule.title`,\n",
    "                sc.reference = row.`schedule.reference`,\n",
    "                sc.uri = row.`schedule.uri`\n",
    "            MERGE (l)-[:HAS_SCHEDULE]->(sc)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "        print(\"Writing Schedule Paragraph Nodes...\")\n",
    "        sched_paras_df = (\n",
    "            schedules_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"sched_id\"),\n",
    "                explode_outer(\"schedule.paragraphs\").alias(\"paragraph\"),\n",
    "            )\n",
    "            .filter(col(\"paragraph\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"para_id\",\n",
    "                coalesce(\n",
    "                    col(\"paragraph.uri\"),\n",
    "                    concat(\n",
    "                        col(\"sched_id\"),\n",
    "                        lit(\"#spara_\"),\n",
    "                        coalesce(\n",
    "                            col(\"paragraph.paragraph_number\"),\n",
    "                            md5(col(\"paragraph\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        sched_paras_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (sc:Schedule {id: row.sched_id})\n",
    "            MERGE (p:ScheduleParagraph {id: row.para_id})\n",
    "            SET p.number = row.`paragraph.paragraph_number`,\n",
    "                p.order = row.`paragraph.order`,\n",
    "                p.crossheading = row.`paragraph.crossheading`,\n",
    "                p.text = row.`paragraph.text`,\n",
    "                p.uri = row.`paragraph.uri`\n",
    "            MERGE (sc)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "        sched_para_comm_df = sched_paras_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"para_id\").alias(\"parent_id\"),\n",
    "            explode_outer(\"paragraph.commentaries\").alias(\"commentary\"),\n",
    "        ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        sched_subpara_comm_df = None\n",
    "        if \"subparagraphs\" in sched_paras_df.schema[\"paragraph\"].dataType.fieldNames():\n",
    "            print(\"Writing Schedule Sub-paragraph Nodes...\")\n",
    "            sched_subparas_df = (\n",
    "                sched_paras_df.select(\n",
    "                    col(\"legis_uri\"),\n",
    "                    col(\"para_id\"),\n",
    "                    explode_outer(\"paragraph.subparagraphs\").alias(\"subparagraph\"),\n",
    "                )\n",
    "                .filter(col(\"subparagraph\").isNotNull())\n",
    "                .withColumn(\n",
    "                    \"subpara_id\",\n",
    "                    coalesce(\n",
    "                        col(\"subparagraph.uri\"),\n",
    "                        concat(\n",
    "                            col(\"para_id\"),\n",
    "                            lit(\"#ssub_\"),\n",
    "                            coalesce(\n",
    "                                col(\"subparagraph.subparagraph_number\"),\n",
    "                                md5(col(\"subparagraph\").cast(\"string\")),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            sched_subparas_df.write.format(\"org.neo4j.spark.DataSource\").mode(\n",
    "                \"Append\"\n",
    "            ).option(\n",
    "                \"query\",\n",
    "                \"\"\"\n",
    "                UNWIND event AS row\n",
    "                MATCH (p:ScheduleParagraph {id: row.para_id})\n",
    "                MERGE (sp:ScheduleSubparagraph {id: row.subpara_id})\n",
    "                SET sp.number = row.`subparagraph.subparagraph_number`,\n",
    "                    sp.order = row.`subparagraph.order`,\n",
    "                    sp.text = row.`subparagraph.text`,\n",
    "                    sp.uri = row.`subparagraph.uri`\n",
    "                MERGE (p)-[:HAS_SUBPARAGRAPH]->(sp)\n",
    "            \"\"\",\n",
    "            ).save()\n",
    "\n",
    "            sched_subpara_comm_df = sched_subparas_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"subpara_id\").alias(\"parent_id\"),\n",
    "                explode_outer(\"subparagraph.commentaries\").alias(\"commentary\"),\n",
    "            ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        return sched_paras_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "\n",
    "    def _write_single_commentary(self, df, parent_label):\n",
    "        if df is not None:\n",
    "            fields = df.schema[\"commentary\"].dataType.fieldNames()\n",
    "            type_col = col(\"commentary.type\") if \"type\" in fields else lit(None)\n",
    "            text_col = col(\"commentary.text\") if \"text\" in fields else lit(None)\n",
    "\n",
    "            flat_df = (\n",
    "                df.select(\n",
    "                    col(\"legis_uri\"),\n",
    "                    col(\"parent_id\"),\n",
    "                    col(\"commentary.ref_id\").alias(\"ref_id\"),\n",
    "                    type_col.alias(\"type\"),\n",
    "                    text_col.alias(\"text\"),\n",
    "                )\n",
    "                .filter(col(\"ref_id\").isNotNull())\n",
    "                .dropDuplicates([\"parent_id\", \"ref_id\"])\n",
    "            )\n",
    "\n",
    "            flat_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "                \"query\",\n",
    "                f\"\"\"\n",
    "                UNWIND event AS row\n",
    "                WITH row WHERE row.ref_id IS NOT NULL\n",
    "                MATCH (parent:{parent_label} {{id: row.parent_id}})\n",
    "                MERGE (com:Commentary {{id: row.legis_uri + \"#\" + row.ref_id}})\n",
    "                SET com.type = row.type, com.text = row.text\n",
    "                MERGE (parent)-[:HAS_COMMENTARY]->(com)\n",
    "            \"\"\",\n",
    "            ).save()\n",
    "\n",
    "    def _write_commentaries(\n",
    "        self, para_comm_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "    ):\n",
    "        print(\"Writing Commentary Nodes...\")\n",
    "        self._write_single_commentary(para_comm_df, \"Paragraph\")\n",
    "        self._write_single_commentary(sched_para_comm_df, \"ScheduleParagraph\")\n",
    "        self._write_single_commentary(sched_subpara_comm_df, \"ScheduleSubparagraph\")\n",
    "\n",
    "    def _write_citations(self, all_comms):\n",
    "        comm_fields = all_comms.schema[\"commentary\"].dataType.fieldNames()\n",
    "        if \"citations\" not in comm_fields:\n",
    "            return\n",
    "\n",
    "        print(\"Writing Citation Nodes... (Sequential & Strict Match)\")\n",
    "        citations_df = (\n",
    "            all_comms.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "                explode_outer(\"commentary.citations\").alias(\"citation\"),\n",
    "            )\n",
    "            .filter(col(\"citation\").isNotNull())\n",
    "            .filter(col(\"citation.uri\").isNotNull())\n",
    "        )\n",
    "\n",
    "        cit_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "\n",
    "        citations_flat = (\n",
    "            citations_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"comm_id\"),\n",
    "                (\n",
    "                    col(\"citation.id\").alias(\"cit_id\")\n",
    "                    if \"id\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_id\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.uri\").alias(\"cit_uri\")\n",
    "                    if \"uri\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_uri\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.title\").alias(\"cit_title\")\n",
    "                    if \"title\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_title\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.year\").alias(\"cit_year\")\n",
    "                    if \"year\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_year\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.class\").alias(\"cit_class\")\n",
    "                    if \"class\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_class\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.text\").alias(\"cit_text\")\n",
    "                    if \"text\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_text\")\n",
    "                ),\n",
    "            )\n",
    "            .filter(col(\"cit_id\").isNotNull())\n",
    "            .dropDuplicates([\"comm_id\", \"cit_id\"])\n",
    "        )\n",
    "\n",
    "        citations_flat = citations_flat.withColumn(\n",
    "            \"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\")\n",
    "        )\n",
    "        citations_flat = citations_flat.coalesce(1)\n",
    "\n",
    "        citations_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL\n",
    "                AND row.cit_id IS NOT NULL\n",
    "                AND row.legis_uri IS NOT NULL\n",
    "            \n",
    "            MATCH (com:Commentary {id: row.legis_uri + \"#\" + row.comm_id})\n",
    "            MERGE (cit:Citation {id: row.legis_uri + \"#\" + row.cit_id})\n",
    "            SET cit.uri = row.cit_uri,\n",
    "                cit.title = row.cit_title,\n",
    "                cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class,\n",
    "                cit.text = row.cit_text\n",
    "            MERGE (com)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            WITH cit, row\n",
    "            WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "    def _write_citation_subrefs(self, all_comms):\n",
    "        comm_fields = all_comms.schema[\"commentary\"].dataType.fieldNames()\n",
    "        if \"citation_subrefs\" not in comm_fields:\n",
    "            return\n",
    "\n",
    "        print(\"Writing Citation SubRefs... (Sequential & Strict Match)\")\n",
    "        subrefs_df = (\n",
    "            all_comms.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "                explode_outer(\"commentary.citation_subrefs\").alias(\"subref\"),\n",
    "            )\n",
    "            .filter(col(\"subref\").isNotNull())\n",
    "            .filter(col(\"subref.uri\").isNotNull())\n",
    "        )\n",
    "\n",
    "        sub_fields = subrefs_df.schema[\"subref\"].dataType.fieldNames()\n",
    "\n",
    "        subrefs_flat = (\n",
    "            subrefs_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"comm_id\"),\n",
    "                (\n",
    "                    col(\"subref.id\").alias(\"sub_id\")\n",
    "                    if \"id\" in sub_fields\n",
    "                    else lit(None).alias(\"sub_id\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"subref.citation_ref\").alias(\"citation_ref\")\n",
    "                    if \"citation_ref\" in sub_fields\n",
    "                    else lit(None).alias(\"citation_ref\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"subref.uri\").alias(\"sub_uri\")\n",
    "                    if \"uri\" in sub_fields\n",
    "                    else lit(None).alias(\"sub_uri\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"subref.section_ref\").alias(\"sub_section_ref\")\n",
    "                    if \"section_ref\" in sub_fields\n",
    "                    else lit(None).alias(\"sub_section_ref\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"subref.text\").alias(\"sub_text\")\n",
    "                    if \"text\" in sub_fields\n",
    "                    else lit(None).alias(\"sub_text\")\n",
    "                ),\n",
    "            )\n",
    "            .filter(col(\"sub_id\").isNotNull())\n",
    "            .dropDuplicates([\"comm_id\", \"sub_id\"])\n",
    "        )\n",
    "\n",
    "        subrefs_flat = subrefs_flat.withColumn(\n",
    "            \"base_uri\",\n",
    "            regexp_replace(\n",
    "                col(\"sub_uri\"),\n",
    "                r\"(http://www\\.legislation\\.gov\\.uk)/id/([^/]+/[0-9]+/[0-9]+).*\",\n",
    "                \"$1/$2\",\n",
    "            ),\n",
    "        )\n",
    "        subrefs_flat = subrefs_flat.coalesce(1)\n",
    "\n",
    "        subrefs_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL\n",
    "                AND row.sub_id IS NOT NULL\n",
    "                AND row.legis_uri IS NOT NULL\n",
    "            \n",
    "            MERGE (sub:CitationSubRef {id: row.legis_uri + \"#\" + row.sub_id})\n",
    "            SET sub.uri = row.sub_uri, \n",
    "                sub.section_ref = row.sub_section_ref, \n",
    "                sub.text = row.sub_text\n",
    "                \n",
    "            WITH sub, row\n",
    "            MATCH (com:Commentary {id: row.legis_uri + \"#\" + row.comm_id})\n",
    "            OPTIONAL MATCH (cit:Citation {id: row.legis_uri + \"#\" + row.citation_ref})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN cit IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (cit)-[:HAS_SUBREF]->(sub)\n",
    "            )\n",
    "            FOREACH (_ IN CASE WHEN cit IS NULL THEN [1] ELSE [] END |\n",
    "                MERGE (com)-[:HAS_SUBREF]->(sub)\n",
    "            )\n",
    "            \n",
    "            WITH sub, row\n",
    "            WHERE row.base_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.base_uri})\n",
    "            MERGE (sub)-[:REFERENCES]->(leg)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "    def _write_super_relationships(self, raw_df):\n",
    "        if \"super\" not in raw_df.columns:\n",
    "            return\n",
    "\n",
    "        print(\"Writing Super Relationships...\")\n",
    "        super_fields = raw_df.schema[\"super\"].dataType.fieldNames()\n",
    "        super_df = raw_df.select(\n",
    "            col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "            (\n",
    "                col(\"super.supersedes\").alias(\"supersedes\")\n",
    "                if \"supersedes\" in super_fields\n",
    "                else lit(None).alias(\"supersedes\")\n",
    "            ),\n",
    "            (\n",
    "                col(\"super.superseded_by\").alias(\"superseded_by\")\n",
    "                if \"superseded_by\" in super_fields\n",
    "                else lit(None).alias(\"superseded_by\")\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        super_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"            UNWIND event AS row\n",
    "            WITH row WHERE row.legis_uri IS NOT NULL\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN row.supersedes IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (target:Legislation {uri: row.supersedes})\n",
    "                MERGE (l)-[:SUPERSEDES]->(target)\n",
    "            )\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN row.superseded_by IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (target:Legislation {uri: row.superseded_by})\n",
    "                MERGE (l)-[:SUPERSEDED_BY]->(target)\n",
    "            )\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "    def _write_explanatory_notes_nodes(self, raw_df):\n",
    "        if \"explanatory_notes\" not in raw_df.columns:\n",
    "            return None\n",
    "\n",
    "        print(\"Writing Explanatory Notes Nodes...\")\n",
    "        notes_base_df = (\n",
    "            raw_df.select(\n",
    "                col(\"legislation_url\").alias(\"legis_uri\"), col(\"explanatory_notes\")\n",
    "            )\n",
    "            .filter(col(\"explanatory_notes\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"notes_id\",\n",
    "                coalesce(\n",
    "                    col(\"explanatory_notes.uri\"),\n",
    "                    md5(col(\"explanatory_notes\").cast(\"string\")),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        notes_base_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri}) \n",
    "            MERGE (en:ExplanatoryNotes {id: row.notes_id})\n",
    "            SET en.uri = row.`explanatory_notes.uri`\n",
    "            MERGE (l)-[:HAS_EXPLANATORY_NOTES]->(en)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "        notes_paras_df = (\n",
    "            notes_base_df.select(\n",
    "                col(\"explanatory_notes.uri\").alias(\"notes_id\"),\n",
    "                col(\"legis_uri\"),\n",
    "                explode_outer(\"explanatory_notes.paragraphs\").alias(\"paragraph\"),\n",
    "            )\n",
    "            .filter(col(\"paragraph\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"para_id\",\n",
    "                concat(\n",
    "                    col(\"notes_id\"),\n",
    "                    lit(\"#enp_\"),\n",
    "                    md5(col(\"paragraph.text\").cast(\"string\")),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        notes_paras_df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (en:ExplanatoryNotes {id: row.notes_id})\n",
    "            MERGE (p:ExplanatoryNotesParagraph {id: row.para_id})\n",
    "            SET p.text = row.`paragraph.text`,\n",
    "                p.uri = row.`paragraph.uri`\n",
    "            MERGE (en)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "        return notes_paras_df\n",
    "\n",
    "    def _write_explanatory_notes_citations(self, notes_paras_df):\n",
    "        if notes_paras_df is None:\n",
    "            return\n",
    "\n",
    "        comm_fields = notes_paras_df.schema[\"paragraph\"].dataType.fieldNames()\n",
    "        if \"citations\" not in comm_fields:\n",
    "            return\n",
    "\n",
    "        print(\"Writing Explanatory Notes Citation Nodes... (Sequential & Strict Match)\")\n",
    "        citations_df = notes_paras_df.select(\n",
    "            col(\"para_id\"), explode_outer(\"paragraph.citations\").alias(\"citation\")\n",
    "        ).filter(col(\"citation\").isNotNull())\n",
    "\n",
    "        cit_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "\n",
    "        citations_flat = (\n",
    "            citations_df.select(\n",
    "                col(\"para_id\"),\n",
    "                (\n",
    "                    col(\"citation.id\").alias(\"cit_id\")\n",
    "                    if \"id\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_id\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.uri\").alias(\"cit_uri\")\n",
    "                    if \"uri\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_uri\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.title\").alias(\"cit_title\")\n",
    "                    if \"title\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_title\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.year\").alias(\"cit_year\")\n",
    "                    if \"year\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_year\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.class\").alias(\"cit_class\")\n",
    "                    if \"class\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_class\")\n",
    "                ),\n",
    "                (\n",
    "                    col(\"citation.text\").alias(\"cit_text\")\n",
    "                    if \"text\" in cit_fields\n",
    "                    else lit(None).alias(\"cit_text\")\n",
    "                ),\n",
    "            )\n",
    "            .filter(col(\"cit_id\").isNotNull())\n",
    "            .dropDuplicates([\"para_id\", \"cit_id\"])\n",
    "        )\n",
    "\n",
    "        citations_flat = citations_flat.withColumn(\n",
    "            \"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\")\n",
    "        )\n",
    "        citations_flat = citations_flat.coalesce(1)\n",
    "\n",
    "        citations_flat.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "            \"query\",\n",
    "            \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.para_id IS NOT NULL AND row.cit_id IS NOT NULL\n",
    "            \n",
    "            MATCH (p:ExplanatoryNotesParagraph {id: row.para_id})\n",
    "            MERGE (cit:Citation {id: row.cit_id})\n",
    "            SET cit.uri = row.cit_uri,\n",
    "                cit.title = row.cit_title,\n",
    "                cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class,\n",
    "                cit.text = row.cit_text\n",
    "            MERGE (p)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            WITH cit, row\n",
    "            WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\",\n",
    "        ).save()\n",
    "\n",
    "    def load_full_hierarchy_to_neo4j(self, json_dir=None):\n",
    "        if json_dir is None:\n",
    "            json_dir = f\"{self.json_output_dir}/*/*.json\"\n",
    "\n",
    "        spark = (\n",
    "            SparkSession.builder.appName(\"Legislation Full Graph Builder\")\n",
    "            .config(\n",
    "                \"spark.jars.packages\",\n",
    "                \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.2_for_spark_3\",\n",
    "            )\n",
    "            .config(\"neo4j.url\", self.uri)\n",
    "            .config(\"neo4j.authentication.basic.username\", self.user)\n",
    "            .config(\"neo4j.authentication.basic.password\", self.password)\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "        raw_df = (\n",
    "            spark.read.option(\"multiline\", \"true\")\n",
    "            .option(\"mode\", \"PERMISSIVE\")\n",
    "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"pathGlobFilter\", \"*.json\")\n",
    "            .json(json_dir)\n",
    "        )\n",
    "\n",
    "        if \"_corrupt_record\" in raw_df.columns:\n",
    "            raw_df = raw_df.filter(col(\"_corrupt_record\").isNull()).drop(\n",
    "                \"_corrupt_record\"\n",
    "            )\n",
    "\n",
    "        raw_df = raw_df.filter(\n",
    "            col(\"legislation_url\").isNotNull() & (col(\"legislation_url\") != \"\")\n",
    "        )\n",
    "\n",
    "        self._write_legislation_nodes(raw_df)\n",
    "        self._write_super_relationships(raw_df)\n",
    "        parts_df = self._write_part_nodes(raw_df)\n",
    "        chapters_df = self._write_chapter_nodes(parts_df)\n",
    "        sections_df = self._write_section_nodes(chapters_df)\n",
    "        paragraphs_df = self._write_paragraph_nodes(sections_df)\n",
    "\n",
    "        sched_paras_df, sched_para_comm_df, sched_subpara_comm_df = (\n",
    "            self._write_schedules_nodes(raw_df)\n",
    "        )\n",
    "        notes_paras_df = self._write_explanatory_notes_nodes(raw_df)\n",
    "        self._write_explanatory_notes_citations(notes_paras_df)\n",
    "\n",
    "        sec_comm_df = sections_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"sec_id\").alias(\"parent_id\"),\n",
    "            explode_outer(\"section.commentaries\").alias(\"commentary\"),\n",
    "        ).filter(col(\"commentary\").isNotNull())\n",
    "        para_comm_df = paragraphs_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"para_id\").alias(\"parent_id\"),\n",
    "            explode_outer(\"paragraph.commentaries\").alias(\"commentary\"),\n",
    "        ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        self._write_commentaries(\n",
    "            para_comm_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "        )\n",
    "\n",
    "        all_comms = sec_comm_df.select(\"legis_uri\", \"commentary\").unionByName(\n",
    "            para_comm_df.select(\"legis_uri\", \"commentary\"), allowMissingColumns=True\n",
    "        )\n",
    "        if sched_para_comm_df is not None:\n",
    "            all_comms = all_comms.unionByName(\n",
    "                sched_para_comm_df.select(\"legis_uri\", \"commentary\"),\n",
    "                allowMissingColumns=True,\n",
    "            )\n",
    "        if sched_subpara_comm_df is not None:\n",
    "            all_comms = all_comms.unionByName(\n",
    "                sched_subpara_comm_df.select(\"legis_uri\", \"commentary\"),\n",
    "                allowMissingColumns=True,\n",
    "            )\n",
    "\n",
    "        self._write_citations(all_comms)\n",
    "        self._write_citation_subrefs(all_comms)\n",
    "\n",
    "        print(\"Graph load complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cedbd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    explode_outer,\n",
    "    concat,\n",
    "    coalesce,\n",
    "    md5,\n",
    "    to_date,\n",
    "    regexp_replace,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class LegislationGraphLoader:\n",
    "    def __init__(self, uri, user, password, json_output_dir):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.json_output_dir = json_output_dir\n",
    "\n",
    "    # ==========================================\n",
    "    # HELPER METHODS (To dry up boilerplate)\n",
    "    # ==========================================\n",
    "\n",
    "    def _write_to_neo4j(self, df, query):\n",
    "        \"\"\"Helper to reduce the 8-line Spark-Neo4j boilerplate to 1 line.\"\"\"\n",
    "        if df is not None:\n",
    "            df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Append\").option(\n",
    "                \"query\", query\n",
    "            ).save()\n",
    "\n",
    "    def _safe_extract(self, struct_col, expected_fields, actual_fields, prefix=\"\"):\n",
    "        \"\"\"Extracts fields safely from a struct, falling back to lit(None) to prevent AnalysisExceptions.\"\"\"\n",
    "        return [\n",
    "            (\n",
    "                col(f\"{struct_col}.{f}\").alias(f\"{prefix}{f}\")\n",
    "                if f in actual_fields\n",
    "                else lit(None).alias(f\"{prefix}{f}\")\n",
    "            )\n",
    "            for f in expected_fields\n",
    "        ]\n",
    "\n",
    "    def _safe_legis_extract(self, df, struct_col, expected_fields):\n",
    "        \"\"\"Helper specifically for root-level legislation metadata/identifier extraction.\"\"\"\n",
    "        if struct_col not in df.columns:\n",
    "            return [\n",
    "                lit(None).cast(\"date\" if is_date else \"string\").alias(alias)\n",
    "                for f, alias, is_date in expected_fields\n",
    "            ]\n",
    "\n",
    "        actual_fields = df.schema[struct_col].dataType.fieldNames()\n",
    "        exprs = []\n",
    "        for f, alias, is_date in expected_fields:\n",
    "            if f in actual_fields:\n",
    "                c = col(f\"{struct_col}.{f}\")\n",
    "                if is_date:\n",
    "                    c = to_date(c, \"yyyy-MM-dd\")\n",
    "                exprs.append(c.alias(alias))\n",
    "            else:\n",
    "                exprs.append(\n",
    "                    lit(None).cast(\"date\" if is_date else \"string\").alias(alias)\n",
    "                )\n",
    "        return exprs\n",
    "\n",
    "    # ==========================================\n",
    "    # NODE & RELATIONSHIP WRITERS\n",
    "    # ==========================================\n",
    "\n",
    "    def _write_legislation_nodes(self, raw_df):\n",
    "        print(\"Writing Legislation Nodes...\")\n",
    "\n",
    "        id_fields = [\n",
    "            (\"title\", \"title\", False),\n",
    "            (\"description\", \"description\", False),\n",
    "            (\"modified\", \"modified_date\", True),\n",
    "            (\"valid_date\", \"valid_date\", True),\n",
    "        ]\n",
    "        meta_fields = [\n",
    "            (\"enactment_date\", \"enactment_date\", True),\n",
    "            (\"status\", \"status\", False),\n",
    "            (\"category\", \"category\", False),\n",
    "            (\"coming_into_force\", \"coming_into_force\", True),\n",
    "        ]\n",
    "\n",
    "        select_exprs = [col(\"legislation_url\").alias(\"uri\")]\n",
    "        select_exprs.extend(self._safe_legis_extract(raw_df, \"identifier\", id_fields))\n",
    "        select_exprs.extend(self._safe_legis_extract(raw_df, \"metadata\", meta_fields))\n",
    "\n",
    "        legis_df = raw_df.select(*select_exprs).dropDuplicates([\"uri\"])\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MERGE (l:Legislation {uri: row.uri})\n",
    "            SET l.title = row.title, \n",
    "                l.description = row.description,\n",
    "                l.modified_date = row.modified_date,\n",
    "                l.valid_date = row.valid_date,\n",
    "                l.enactment_date = row.enactment_date,\n",
    "                l.status = row.status,\n",
    "                l.category = row.category,\n",
    "                l.coming_into_force = row.coming_into_force\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(legis_df, query)\n",
    "\n",
    "    def _write_part_nodes(self, raw_df):\n",
    "        print(\"Writing Part Nodes...\")\n",
    "        parts_df = (\n",
    "            raw_df.select(\n",
    "                col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "                explode_outer(\"parts\").alias(\"part\"),\n",
    "            )\n",
    "            .filter(col(\"part\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"part_id\",\n",
    "                concat(\n",
    "                    col(\"legis_uri\"),\n",
    "                    lit(\"#part_\"),\n",
    "                    coalesce(col(\"part.part_number\"), md5(col(\"part\").cast(\"string\"))),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (p:Part {id: row.part_id})\n",
    "            SET p.number = row.`part.part_number`, p.order = row.`part.order`, p.title = row.`part.title`,\n",
    "                p.uri = row.`part.uri`, p.status = row.`part.status`,\n",
    "                p.restrict_start_date = row.`part.restrict_start_date`, p.restrict_end_date = row.`part.restrict_end_date`\n",
    "            MERGE (l)-[:HAS_PART]->(p)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(parts_df, query)\n",
    "        return parts_df\n",
    "\n",
    "    def _write_chapter_nodes(self, parts_df):\n",
    "        print(\"Writing Chapter Nodes...\")\n",
    "        chapters_df = (\n",
    "            parts_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"part_id\"),\n",
    "                explode_outer(\"part.chapters\").alias(\"chapter\"),\n",
    "            )\n",
    "            .filter(col(\"chapter\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"chapter_id\",\n",
    "                coalesce(\n",
    "                    col(\"chapter.uri\"),\n",
    "                    concat(\n",
    "                        col(\"part_id\"),\n",
    "                        lit(\"#chapter_\"),\n",
    "                        coalesce(\n",
    "                            col(\"chapter.chapter_number\"),\n",
    "                            md5(col(\"chapter\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (p:Part {id: row.part_id})\n",
    "            MERGE (c:Chapter {id: row.chapter_id})\n",
    "            SET c.number = row.`chapter.chapter_number`, c.order = row.`chapter.order`, c.title = row.`chapter.title`,\n",
    "                c.uri = row.`chapter.uri`, c.status = row.`chapter.status`,\n",
    "                c.restrict_start_date = date(row.`chapter.restrict_start_date`), c.restrict_end_date = date(row.`chapter.restrict_end_date`)\n",
    "            MERGE (p)-[:HAS_CHAPTER]->(c)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(chapters_df, query)\n",
    "        return chapters_df\n",
    "\n",
    "    def _write_section_nodes(self, chapters_df):\n",
    "        print(\"Writing Section Nodes...\")\n",
    "        sections_df = (\n",
    "            chapters_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"chapter_id\"),\n",
    "                explode_outer(\"chapter.sections\").alias(\"section\"),\n",
    "            )\n",
    "            .filter(col(\"section\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"sec_id\",\n",
    "                coalesce(\n",
    "                    col(\"section.uri\"),\n",
    "                    concat(\n",
    "                        col(\"chapter_id\"),\n",
    "                        lit(\"#sec_\"),\n",
    "                        coalesce(\n",
    "                            col(\"section.section_number\"),\n",
    "                            md5(col(\"section\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (c:Chapter {id: row.chapter_id})\n",
    "            MERGE (s:Section {id: row.sec_id})\n",
    "            SET s.number = row.`section.section_number`, s.order = row.`section.order`, s.title = row.`section.title`, \n",
    "                s.uri = row.`section.uri`, s.restrict_extent = row.`section.restrict_extent`,\n",
    "                s.restrict_start_date = date(row.`section.restrict_start_date`), s.restrict_end_date = date(row.`section.restrict_end_date`)\n",
    "            MERGE (c)-[:HAS_SECTION]->(s)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(sections_df, query)\n",
    "        return sections_df\n",
    "\n",
    "    def _write_paragraph_nodes(self, sections_df):\n",
    "        print(\"Writing Paragraph Nodes...\")\n",
    "        paragraphs_df = (\n",
    "            sections_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"sec_id\"),\n",
    "                explode_outer(\"section.paragraphs\").alias(\"paragraph\"),\n",
    "            )\n",
    "            .filter(col(\"paragraph\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"para_id\",\n",
    "                coalesce(\n",
    "                    col(\"paragraph.uri\"),\n",
    "                    concat(\n",
    "                        col(\"sec_id\"),\n",
    "                        lit(\"#para_\"),\n",
    "                        coalesce(\n",
    "                            col(\"paragraph.paragraph_number\"),\n",
    "                            md5(col(\"paragraph\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (s:Section {id: row.sec_id})\n",
    "            MERGE (pa:Paragraph {id: row.para_id})\n",
    "            SET pa.number = row.`paragraph.paragraph_number`, pa.order = row.`paragraph.order`,\n",
    "                pa.text = row.`paragraph.text`, pa.uri = row.`paragraph.uri`\n",
    "            MERGE (s)-[:HAS_PARAGRAPH]->(pa)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(paragraphs_df, query)\n",
    "        return paragraphs_df\n",
    "\n",
    "    def _write_schedules_nodes(self, raw_df):\n",
    "        if (\n",
    "            \"schedules\" not in raw_df.columns\n",
    "            or raw_df.schema[\"schedules\"].dataType.simpleString() == \"array<string>\"\n",
    "        ):\n",
    "            return None, None, None\n",
    "\n",
    "        print(\"Writing Schedule Nodes...\")\n",
    "        schedules_df = (\n",
    "            raw_df.select(\n",
    "                col(\"legislation_url\").alias(\"legis_uri\"),\n",
    "                explode_outer(\"schedules\").alias(\"schedule\"),\n",
    "            )\n",
    "            .filter(col(\"schedule\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"sched_id\",\n",
    "                coalesce(\n",
    "                    col(\"schedule.uri\"),\n",
    "                    concat(\n",
    "                        col(\"legis_uri\"),\n",
    "                        lit(\"#sched_\"),\n",
    "                        coalesce(\n",
    "                            col(\"schedule.schedule_number\"),\n",
    "                            md5(col(\"schedule\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query_sched = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            MERGE (sc:Schedule {id: row.sched_id})\n",
    "            SET sc.number = row.`schedule.schedule_number`, sc.order = row.`schedule.order`,\n",
    "                sc.title = row.`schedule.title`, sc.reference = row.`schedule.reference`, sc.uri = row.`schedule.uri`\n",
    "            MERGE (l)-[:HAS_SCHEDULE]->(sc)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(schedules_df, query_sched)\n",
    "\n",
    "        print(\"Writing Schedule Paragraph Nodes...\")\n",
    "        sched_paras_df = (\n",
    "            schedules_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"sched_id\"),\n",
    "                explode_outer(\"schedule.paragraphs\").alias(\"paragraph\"),\n",
    "            )\n",
    "            .filter(col(\"paragraph\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"para_id\",\n",
    "                coalesce(\n",
    "                    col(\"paragraph.uri\"),\n",
    "                    concat(\n",
    "                        col(\"sched_id\"),\n",
    "                        lit(\"#spara_\"),\n",
    "                        coalesce(\n",
    "                            col(\"paragraph.paragraph_number\"),\n",
    "                            md5(col(\"paragraph\").cast(\"string\")),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query_para = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (sc:Schedule {id: row.sched_id})\n",
    "            MERGE (p:ScheduleParagraph {id: row.para_id})\n",
    "            SET p.number = row.`paragraph.paragraph_number`, p.order = row.`paragraph.order`,\n",
    "                p.crossheading = row.`paragraph.crossheading`, p.text = row.`paragraph.text`, p.uri = row.`paragraph.uri`\n",
    "            MERGE (sc)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(sched_paras_df, query_para)\n",
    "\n",
    "        sched_para_comm_df = sched_paras_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"para_id\").alias(\"parent_id\"),\n",
    "            explode_outer(\"paragraph.commentaries\").alias(\"commentary\"),\n",
    "        ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        sched_subpara_comm_df = None\n",
    "        if \"subparagraphs\" in sched_paras_df.schema[\"paragraph\"].dataType.fieldNames():\n",
    "            print(\"Writing Schedule Sub-paragraph Nodes...\")\n",
    "            sched_subparas_df = (\n",
    "                sched_paras_df.select(\n",
    "                    col(\"legis_uri\"),\n",
    "                    col(\"para_id\"),\n",
    "                    explode_outer(\"paragraph.subparagraphs\").alias(\"subparagraph\"),\n",
    "                )\n",
    "                .filter(col(\"subparagraph\").isNotNull())\n",
    "                .withColumn(\n",
    "                    \"subpara_id\",\n",
    "                    coalesce(\n",
    "                        col(\"subparagraph.uri\"),\n",
    "                        concat(\n",
    "                            col(\"para_id\"),\n",
    "                            lit(\"#ssub_\"),\n",
    "                            coalesce(\n",
    "                                col(\"subparagraph.subparagraph_number\"),\n",
    "                                md5(col(\"subparagraph\").cast(\"string\")),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            query_sub = \"\"\"\n",
    "                UNWIND event AS row\n",
    "                MATCH (p:ScheduleParagraph {id: row.para_id})\n",
    "                MERGE (sp:ScheduleSubparagraph {id: row.subpara_id})\n",
    "                SET sp.number = row.`subparagraph.subparagraph_number`, sp.order = row.`subparagraph.order`,\n",
    "                    sp.text = row.`subparagraph.text`, sp.uri = row.`subparagraph.uri`\n",
    "                MERGE (p)-[:HAS_SUBPARAGRAPH]->(sp)\n",
    "            \"\"\"\n",
    "            self._write_to_neo4j(sched_subparas_df, query_sub)\n",
    "\n",
    "            sched_subpara_comm_df = sched_subparas_df.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"subpara_id\").alias(\"parent_id\"),\n",
    "                explode_outer(\"subparagraph.commentaries\").alias(\"commentary\"),\n",
    "            ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        return sched_paras_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "\n",
    "    def _write_single_commentary(self, df, parent_label):\n",
    "        if df is not None:\n",
    "            actual_fields = df.schema[\"commentary\"].dataType.fieldNames()\n",
    "            safe_cols = [\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"parent_id\"),\n",
    "                col(\"commentary.ref_id\").alias(\"ref_id\"),\n",
    "            ] + self._safe_extract(\"commentary\", [\"type\", \"text\"], actual_fields)\n",
    "\n",
    "            flat_df = (\n",
    "                df.select(*safe_cols)\n",
    "                .filter(col(\"ref_id\").isNotNull())\n",
    "                .dropDuplicates([\"parent_id\", \"ref_id\"])\n",
    "            )\n",
    "\n",
    "            query = f\"\"\"\n",
    "                UNWIND event AS row\n",
    "                WITH row WHERE row.ref_id IS NOT NULL AND row.legis_uri IS NOT NULL\n",
    "                MATCH (parent:{parent_label} {{id: row.parent_id}})\n",
    "                MERGE (com:Commentary {{id: row.legis_uri + \"#\" + row.ref_id}})\n",
    "                SET com.type = row.type, com.text = row.text\n",
    "                MERGE (parent)-[:HAS_COMMENTARY]->(com)\n",
    "            \"\"\"\n",
    "            self._write_to_neo4j(flat_df, query)\n",
    "\n",
    "    def _write_commentaries(\n",
    "        self, para_comm_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "    ):\n",
    "        print(\"Writing Commentary Nodes...\")\n",
    "        self._write_single_commentary(para_comm_df, \"Paragraph\")\n",
    "        self._write_single_commentary(sched_para_comm_df, \"ScheduleParagraph\")\n",
    "        self._write_single_commentary(sched_subpara_comm_df, \"ScheduleSubparagraph\")\n",
    "\n",
    "    def _write_citations(self, all_comms):\n",
    "        if \"citations\" not in all_comms.schema[\"commentary\"].dataType.fieldNames():\n",
    "            return\n",
    "\n",
    "        print(\"Writing Citation Nodes... (Sequential & Strict Match)\")\n",
    "        citations_df = (\n",
    "            all_comms.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "                explode_outer(\"commentary.citations\").alias(\"citation\"),\n",
    "            )\n",
    "            .filter(col(\"citation\").isNotNull())\n",
    "            .filter(col(\"citation.uri\").isNotNull())\n",
    "        )\n",
    "\n",
    "        expected_fields = [\"id\", \"uri\", \"title\", \"year\", \"class\", \"text\"]\n",
    "        actual_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "        safe_cols = [col(\"legis_uri\"), col(\"comm_id\")] + self._safe_extract(\n",
    "            \"citation\", expected_fields, actual_fields, \"cit_\"\n",
    "        )\n",
    "\n",
    "        citations_flat = (\n",
    "            citations_df.select(*safe_cols)\n",
    "            .filter(col(\"cit_id\").isNotNull())\n",
    "            .dropDuplicates([\"comm_id\", \"cit_id\"])\n",
    "            .withColumn(\"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\"))\n",
    "            .coalesce(1)\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL AND row.cit_id IS NOT NULL AND row.legis_uri IS NOT NULL\n",
    "            \n",
    "            MATCH (com:Commentary {id: row.legis_uri + \"#\" + row.comm_id})\n",
    "            MERGE (cit:Citation {id: row.legis_uri + \"#\" + row.cit_id})\n",
    "            SET cit.uri = row.cit_uri, cit.title = row.cit_title, cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class, cit.text = row.cit_text\n",
    "            MERGE (com)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            WITH cit, row WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(citations_flat, query)\n",
    "\n",
    "    def _write_citation_subrefs(self, all_comms):\n",
    "        if (\n",
    "            \"citation_subrefs\"\n",
    "            not in all_comms.schema[\"commentary\"].dataType.fieldNames()\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        print(\"Writing Citation SubRefs... (Sequential & Strict Match)\")\n",
    "        subrefs_df = (\n",
    "            all_comms.select(\n",
    "                col(\"legis_uri\"),\n",
    "                col(\"commentary.ref_id\").alias(\"comm_id\"),\n",
    "                explode_outer(\"commentary.citation_subrefs\").alias(\"subref\"),\n",
    "            )\n",
    "            .filter(col(\"subref\").isNotNull())\n",
    "            .filter(col(\"subref.uri\").isNotNull())\n",
    "        )\n",
    "\n",
    "        expected_fields = [\"id\", \"citation_ref\", \"uri\", \"section_ref\", \"text\"]\n",
    "        actual_fields = subrefs_df.schema[\"subref\"].dataType.fieldNames()\n",
    "        safe_cols = [col(\"legis_uri\"), col(\"comm_id\")] + self._safe_extract(\n",
    "            \"subref\", expected_fields, actual_fields, \"sub_\"\n",
    "        )\n",
    "\n",
    "        subrefs_flat = (\n",
    "            subrefs_df.select(*safe_cols)\n",
    "            .filter(col(\"sub_id\").isNotNull())\n",
    "            .dropDuplicates([\"comm_id\", \"sub_id\"])\n",
    "            .withColumn(\n",
    "                \"base_uri\",\n",
    "                regexp_replace(\n",
    "                    col(\"sub_uri\"),\n",
    "                    r\"(http://www\\.legislation\\.gov\\.uk)/id/([^/]+/[0-9]+/[0-9]+).*\",\n",
    "                    \"$1/$2\",\n",
    "                ),\n",
    "            )\n",
    "            .coalesce(1)\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.comm_id IS NOT NULL AND row.sub_id IS NOT NULL AND row.legis_uri IS NOT NULL\n",
    "            \n",
    "            MERGE (sub:CitationSubRef {id: row.legis_uri + \"#\" + row.sub_id})\n",
    "            SET sub.uri = row.sub_uri, sub.section_ref = row.sub_section_ref, sub.text = row.sub_text\n",
    "                \n",
    "            WITH sub, row\n",
    "            MATCH (com:Commentary {id: row.legis_uri + \"#\" + row.comm_id})\n",
    "            OPTIONAL MATCH (cit:Citation {id: row.legis_uri + \"#\" + row.sub_citation_ref})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN cit IS NOT NULL THEN [1] ELSE [] END | MERGE (cit)-[:HAS_SUBREF]->(sub))\n",
    "            FOREACH (_ IN CASE WHEN cit IS NULL THEN [1] ELSE [] END | MERGE (com)-[:HAS_SUBREF]->(sub))\n",
    "            \n",
    "            WITH sub, row WHERE row.base_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.base_uri})\n",
    "            MERGE (sub)-[:REFERENCES]->(leg)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(subrefs_flat, query)\n",
    "\n",
    "    def _write_super_relationships(self, raw_df):\n",
    "        if \"super\" not in raw_df.columns:\n",
    "            return\n",
    "\n",
    "        print(\"Writing Super Relationships...\")\n",
    "        actual_fields = raw_df.schema[\"super\"].dataType.fieldNames()\n",
    "        safe_cols = [col(\"legislation_url\").alias(\"legis_uri\")] + self._safe_extract(\n",
    "            \"super\", [\"supersedes\", \"superseded_by\"], actual_fields\n",
    "        )\n",
    "\n",
    "        super_df = raw_df.select(*safe_cols)\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.legis_uri IS NOT NULL\n",
    "            MATCH (l:Legislation {uri: row.legis_uri})\n",
    "            \n",
    "            FOREACH (_ IN CASE WHEN row.supersedes IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (target:Legislation {uri: row.supersedes})\n",
    "                MERGE (l)-[:SUPERSEDES]->(target)\n",
    "            )\n",
    "            FOREACH (_ IN CASE WHEN row.superseded_by IS NOT NULL THEN [1] ELSE [] END |\n",
    "                MERGE (target:Legislation {uri: row.superseded_by})\n",
    "                MERGE (l)-[:SUPERSEDED_BY]->(target)\n",
    "            )\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(super_df, query)\n",
    "\n",
    "    def _write_explanatory_notes_nodes(self, raw_df):\n",
    "        if \"explanatory_notes\" not in raw_df.columns:\n",
    "            return None\n",
    "\n",
    "        print(\"Writing Explanatory Notes Nodes...\")\n",
    "        notes_base_df = (\n",
    "            raw_df.select(\n",
    "                col(\"legislation_url\").alias(\"legis_uri\"), col(\"explanatory_notes\")\n",
    "            )\n",
    "            .filter(col(\"explanatory_notes\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"notes_id\",\n",
    "                coalesce(\n",
    "                    col(\"explanatory_notes.uri\"),\n",
    "                    concat(\n",
    "                        col(\"legis_uri\"),\n",
    "                        lit(\"#en_\"),\n",
    "                        md5(col(\"explanatory_notes\").cast(\"string\")),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query_base = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (l:Legislation {uri: row.legis_uri}) \n",
    "            MERGE (en:ExplanatoryNotes {id: row.notes_id})\n",
    "            SET en.uri = row.`explanatory_notes.uri`\n",
    "            MERGE (l)-[:HAS_EXPLANATORY_NOTES]->(en)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(notes_base_df, query_base)\n",
    "\n",
    "        notes_paras_df = (\n",
    "            notes_base_df.select(\n",
    "                col(\"notes_id\"),\n",
    "                col(\"legis_uri\"),\n",
    "                explode_outer(\"explanatory_notes.paragraphs\").alias(\"paragraph\"),\n",
    "            )\n",
    "            .filter(col(\"paragraph\").isNotNull())\n",
    "            .withColumn(\n",
    "                \"para_id\",\n",
    "                concat(\n",
    "                    col(\"notes_id\"),\n",
    "                    lit(\"#enp_\"),\n",
    "                    md5(col(\"paragraph.text\").cast(\"string\")),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        query_paras = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            MATCH (en:ExplanatoryNotes {id: row.notes_id})\n",
    "            MERGE (p:ExplanatoryNotesParagraph {id: row.para_id})\n",
    "            SET p.text = row.`paragraph.text`, p.uri = row.`paragraph.uri`\n",
    "            MERGE (en)-[:HAS_PARAGRAPH]->(p)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(notes_paras_df, query_paras)\n",
    "        return notes_paras_df\n",
    "\n",
    "    def _write_explanatory_notes_citations(self, notes_paras_df):\n",
    "        if (\n",
    "            notes_paras_df is None\n",
    "            or \"citations\"\n",
    "            not in notes_paras_df.schema[\"paragraph\"].dataType.fieldNames()\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        print(\"Writing Explanatory Notes Citation Nodes...\")\n",
    "        citations_df = notes_paras_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"para_id\"),\n",
    "            explode_outer(\"paragraph.citations\").alias(\"citation\"),\n",
    "        ).filter(col(\"citation\").isNotNull())\n",
    "\n",
    "        expected_fields = [\"id\", \"uri\", \"title\", \"year\", \"class\", \"text\"]\n",
    "        actual_fields = citations_df.schema[\"citation\"].dataType.fieldNames()\n",
    "        safe_cols = [col(\"legis_uri\"), col(\"para_id\")] + self._safe_extract(\n",
    "            \"citation\", expected_fields, actual_fields, \"cit_\"\n",
    "        )\n",
    "\n",
    "        citations_flat = (\n",
    "            citations_df.select(*safe_cols)\n",
    "            .filter(col(\"cit_id\").isNotNull())\n",
    "            .dropDuplicates([\"para_id\", \"cit_id\"])\n",
    "            .withColumn(\"norm_uri\", regexp_replace(col(\"cit_uri\"), r\"/id/\", \"/\"))\n",
    "            .coalesce(1)\n",
    "        )\n",
    "\n",
    "        query = \"\"\"\n",
    "            UNWIND event AS row\n",
    "            WITH row WHERE row.para_id IS NOT NULL AND row.cit_id IS NOT NULL AND row.legis_uri IS NOT NULL\n",
    "            \n",
    "            MATCH (p:ExplanatoryNotesParagraph {id: row.para_id})\n",
    "            MERGE (cit:Citation {id: row.legis_uri + \"#\" + row.cit_id})\n",
    "            SET cit.uri = row.cit_uri, cit.title = row.cit_title, cit.year = row.cit_year,\n",
    "                cit.class = row.cit_class, cit.text = row.cit_text\n",
    "            MERGE (p)-[:HAS_CITATION]->(cit)\n",
    "            \n",
    "            WITH cit, row WHERE row.norm_uri IS NOT NULL\n",
    "            MATCH (leg:Legislation {uri: row.norm_uri})\n",
    "            MERGE (cit)-[:CITES_ACT]->(leg)\n",
    "        \"\"\"\n",
    "        self._write_to_neo4j(citations_flat, query)\n",
    "\n",
    "    def load_full_hierarchy_to_neo4j(self, json_dir=None):\n",
    "        if json_dir is None:\n",
    "            json_dir = f\"{self.json_output_dir}/*/*.json\"\n",
    "\n",
    "        spark = (\n",
    "            SparkSession.builder.appName(\"Legislation Full Graph Builder\")\n",
    "            .config(\n",
    "                \"spark.jars.packages\",\n",
    "                \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.2_for_spark_3\",\n",
    "            )\n",
    "            .config(\"neo4j.url\", self.uri)\n",
    "            .config(\"neo4j.authentication.basic.username\", self.user)\n",
    "            .config(\"neo4j.authentication.basic.password\", self.password)\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "        raw_df = (\n",
    "            spark.read.option(\"multiline\", \"true\")\n",
    "            .option(\"mode\", \"PERMISSIVE\")\n",
    "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"pathGlobFilter\", \"*.json\")\n",
    "            .json(json_dir)\n",
    "        )\n",
    "\n",
    "        if \"_corrupt_record\" in raw_df.columns:\n",
    "            raw_df = raw_df.filter(col(\"_corrupt_record\").isNull()).drop(\n",
    "                \"_corrupt_record\"\n",
    "            )\n",
    "\n",
    "        raw_df = raw_df.filter(\n",
    "            col(\"legislation_url\").isNotNull() & (col(\"legislation_url\") != \"\")\n",
    "        )\n",
    "\n",
    "        self._write_legislation_nodes(raw_df)\n",
    "        self._write_super_relationships(raw_df)\n",
    "        parts_df = self._write_part_nodes(raw_df)\n",
    "        chapters_df = self._write_chapter_nodes(parts_df)\n",
    "        sections_df = self._write_section_nodes(chapters_df)\n",
    "        paragraphs_df = self._write_paragraph_nodes(sections_df)\n",
    "\n",
    "        sched_paras_df, sched_para_comm_df, sched_subpara_comm_df = (\n",
    "            self._write_schedules_nodes(raw_df)\n",
    "        )\n",
    "        notes_paras_df = self._write_explanatory_notes_nodes(raw_df)\n",
    "        self._write_explanatory_notes_citations(notes_paras_df)\n",
    "\n",
    "        sec_comm_df = sections_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"sec_id\").alias(\"parent_id\"),\n",
    "            explode_outer(\"section.commentaries\").alias(\"commentary\"),\n",
    "        ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        para_comm_df = paragraphs_df.select(\n",
    "            col(\"legis_uri\"),\n",
    "            col(\"para_id\").alias(\"parent_id\"),\n",
    "            explode_outer(\"paragraph.commentaries\").alias(\"commentary\"),\n",
    "        ).filter(col(\"commentary\").isNotNull())\n",
    "\n",
    "        self._write_commentaries(\n",
    "            para_comm_df, sched_para_comm_df, sched_subpara_comm_df\n",
    "        )\n",
    "\n",
    "        all_comms = sec_comm_df.select(\"legis_uri\", \"commentary\").unionByName(\n",
    "            para_comm_df.select(\"legis_uri\", \"commentary\"), allowMissingColumns=True\n",
    "        )\n",
    "        if sched_para_comm_df is not None:\n",
    "            all_comms = all_comms.unionByName(\n",
    "                sched_para_comm_df.select(\"legis_uri\", \"commentary\"),\n",
    "                allowMissingColumns=True,\n",
    "            )\n",
    "        if sched_subpara_comm_df is not None:\n",
    "            all_comms = all_comms.unionByName(\n",
    "                sched_subpara_comm_df.select(\"legis_uri\", \"commentary\"),\n",
    "                allowMissingColumns=True,\n",
    "            )\n",
    "\n",
    "        self._write_citations(all_comms)\n",
    "        self._write_citation_subrefs(all_comms)\n",
    "\n",
    "        print(\"Graph load complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5af65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "def setup_neo4j_constraints(uri, user, password, database):\n",
    "    \"\"\"\n",
    "    Connects directly to Neo4j to ensure unique constraints and indexes exist\n",
    "    before Spark starts pushing data. This prevents duplicate nodes and makes MERGE fast.\n",
    "    \"\"\"\n",
    "    print(\"Setting up Neo4j constraints...\")\n",
    "    constraints = [\n",
    "        \"CREATE CONSTRAINT leg_uri_unique IF NOT EXISTS FOR (l:Legislation) REQUIRE l.uri IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT part_id_unique IF NOT EXISTS FOR (p:Part) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT chap_id_unique IF NOT EXISTS FOR (c:Chapter) REQUIRE c.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sec_id_unique IF NOT EXISTS FOR (s:Section) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT para_id_unique IF NOT EXISTS FOR (pa:Paragraph) REQUIRE pa.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_id_unique IF NOT EXISTS FOR (s:Schedule) REQUIRE s.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_para_id_unique IF NOT EXISTS FOR (p:ScheduleParagraph) REQUIRE p.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sched_subpara_id_unique IF NOT EXISTS FOR (sp:ScheduleSubparagraph) REQUIRE sp.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT com_id_unique IF NOT EXISTS FOR (com:Commentary) REQUIRE com.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT cit_id_unique IF NOT EXISTS FOR (cit:Citation) REQUIRE cit.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT sub_id_unique IF NOT EXISTS FOR (sub:CitationSubRef) REQUIRE sub.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT en_id_unique IF NOT EXISTS FOR (en:ExplanatoryNotes) REQUIRE en.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT ep_id_unique IF NOT EXISTS FOR (ep:ExplanatoryNotesParagraph) REQUIRE ep.id IS UNIQUE;\",\n",
    "    ]\n",
    "\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session(database=database) as session:\n",
    "        for query in constraints:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "    print(\"Constraints successfully applied.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5e23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neo4j constraints...\n",
      "Constraints successfully applied.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 14:40:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Legislation Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Super Relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Part Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Chapter Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Section Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Schedule Sub-paragraph Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Explanatory Notes Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Explanatory Notes Citation Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Commentary Nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation Nodes... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 14:50:08 WARN DAGScheduler: Broadcasting large task binary with size 1276.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Citation SubRefs... (Sequential & Strict Match)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 14:51:01 WARN DAGScheduler: Broadcasting large task binary with size 1273.1 KiB\n",
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph load complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "setup_neo4j_constraints(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, NEO4J_DATABASE)\n",
    "loader = LegislationGraphLoader(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, JSON_OUTPUT_DIR)\n",
    "loader.load_full_hierarchy_to_neo4j()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-legislation-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
